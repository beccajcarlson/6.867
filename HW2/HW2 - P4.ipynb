{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem 4\n",
    "# 4.1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "import math\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import pylab as pl\n",
    "import random\n",
    "import pdb\n",
    "import sklearn as sk\n",
    "from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svmgauss(X, Y, C, gamma, tol):\n",
    "    Xt = X.T\n",
    "    Yt = Y.T\n",
    "    # Data range\n",
    "    a = min(X[0])-0.5\n",
    "    b = max(X[0])+0.5\n",
    "    c = min(X[1])-0.5\n",
    "    d = max(X[1])+0.5\n",
    "    \n",
    "    # matrices for solver\n",
    "    A = matrix(Yt.T, tc = 'd')\n",
    "    b_mat = matrix(0, tc = 'd')\n",
    "    G = matrix(np.vstack(((-1*np.identity(len(Yt))), np.identity(len(Yt)))))\n",
    "    h = matrix(np.hstack((np.zeros(len(Yt)), np.repeat(C, len(Yt))))[np.newaxis].T)\n",
    "    q = matrix(np.repeat(-1, len(Yt))[np.newaxis].T, tc = 'd')\n",
    "    pairwise_sq_dists = squareform(pdist(Xt, 'sqeuclidean'))\n",
    "    K = Yt*Y*scipy.exp(-gamma*pairwise_sq_dists)\n",
    "    P = matrix(K, tc = 'd')\n",
    "    \n",
    "    # Call solver\n",
    "    sol = solvers.qp(P,q,G,h,A,b_mat)\n",
    "    alpha = np.asarray((sol['x']))\n",
    "    status = sol['status']\n",
    "    # Threshold alpha\n",
    "    tol = tol\n",
    "    alpha[alpha < tol] = 0\n",
    "    theta = np.dot(scipy.exp(-gamma*pairwise_sq_dists), np.multiply(alpha, Yt))\n",
    "    cond = (alpha > tol).reshape(-1)\n",
    "    cond2 = ((alpha < C) > tol).reshape(-1)\n",
    "    Xt_sv = Xt[cond]\n",
    "    sq_dists = (cdist(Xt_sv, Xt_sv, 'sqeuclidean'))\n",
    "    Z = np.dot(scipy.exp(-gamma*sq_dists), np.multiply(alpha[cond], Yt[cond]))    \n",
    "    SV = Xt[cond] #support vectors\n",
    "    bias_vec = Yt[cond]-Z    \n",
    "    test=np.histogram(bias_vec, bins=50000)\n",
    "    max(test[0])\n",
    "    bias = np.mean(test[1][test[0]==max(test[0])]) #most frequent bias\n",
    "    bias\n",
    "    return cond, C, gamma, SV, alpha, bias, a, b, c, d, bias_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 400)\n",
      "(1, 400)\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.6124e-03 -6.4829e-03  8e+02  3e+01  6e-12\n",
      " 1: -2.5899e-03 -6.2414e-03  9e+00  3e-01  4e-12\n",
      " 2: -1.0211e-03 -4.4782e-03  3e-01  9e-03  5e-12\n",
      " 3: -2.2269e-04 -3.9258e-03  2e-02  5e-04  9e-13\n",
      " 4: -1.0208e-04 -2.0298e-03  4e-03  7e-05  2e-13\n",
      " 5: -5.3402e-05 -1.0050e-03  2e-03  3e-05  7e-14\n",
      " 6: -1.7373e-05 -5.0635e-04  7e-04  1e-05  3e-14\n",
      " 7: -1.0726e-05 -9.9728e-05  1e-04  2e-06  1e-14\n",
      " 8: -8.4522e-06 -2.0448e-05  1e-05  1e-07  7e-15\n",
      " 9: -9.5261e-06 -1.4312e-05  6e-06  4e-08  5e-15\n",
      "10: -1.0208e-05 -1.1550e-05  1e-06  4e-20  6e-15\n",
      "11: -1.0639e-05 -1.0823e-05  2e-07  4e-20  6e-15\n",
      "12: -1.0711e-05 -1.0720e-05  9e-09  4e-20  6e-15\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:32: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 50001 but corresponding boolean dimension is 50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0576e+02 -2.2605e+01  1e+03  4e+01  4e-16\n",
      " 1: -1.9359e+01 -6.6897e-01  8e+01  2e+00  3e-16\n",
      " 2: -2.0679e-01 -7.8080e-03  9e-01  3e-02  5e-16\n",
      " 3: -4.9570e-03 -7.6991e-03  1e-02  2e-04  5e-16\n",
      " 4: -3.6575e-03 -5.0654e-03  1e-03  2e-17  4e-16\n",
      " 5: -3.9797e-03 -4.0111e-03  3e-05  1e-17  4e-16\n",
      " 6: -3.9998e-03 -4.0001e-03  3e-07  9e-18  9e-17\n",
      " 7: -4.0000e-03 -4.0000e-03  3e-09  2e-17  2e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.6135e-03 -4.0093e+00  8e+02  3e+01  7e-12\n",
      " 1: -2.5992e-03 -3.9313e+00  1e+01  3e-01  5e-12\n",
      " 2: -1.7679e-03 -1.3405e+00  2e+00  2e-02  5e-12\n",
      " 3: -3.3352e-04 -1.8061e-01  2e-01  2e-03  3e-12\n",
      " 4: -1.4847e-04 -2.3215e-02  3e-02  3e-04  6e-13\n",
      " 5: -5.0300e-05 -4.3055e-03  5e-03  4e-05  1e-13\n",
      " 6: -2.2654e-05 -1.1990e-03  1e-03  1e-05  4e-14\n",
      " 7: -1.1648e-05 -2.2463e-04  3e-04  2e-06  1e-14\n",
      " 8: -8.3187e-06 -4.3644e-05  4e-05  2e-07  7e-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:37: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 50001 but corresponding boolean dimension is 50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9: -9.4332e-06 -2.0045e-05  1e-05  6e-08  6e-15\n",
      "10: -9.5643e-06 -1.2920e-05  3e-06  3e-17  6e-15\n",
      "11: -1.0419e-05 -1.1215e-05  8e-07  4e-17  6e-15\n",
      "12: -1.0659e-05 -1.0794e-05  1e-07  4e-17  7e-15\n",
      "13: -1.0713e-05 -1.0717e-05  3e-09  4e-17  6e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0659e+02 -2.9665e+01  1e+03  4e+01  1e-16\n",
      " 1: -2.1635e+01 -8.2761e+00  8e+01  2e+00  3e-16\n",
      " 2: -3.5356e+00 -7.2306e+00  4e+00  2e-14  7e-16\n",
      " 3: -3.8682e+00 -4.0180e+00  1e-01  1e-14  2e-16\n",
      " 4: -3.9758e+00 -3.9773e+00  2e-03  1e-14  3e-16\n",
      " 5: -3.9769e+00 -3.9769e+00  2e-05  6e-15  1e-16\n",
      " 6: -3.9769e+00 -3.9769e+00  2e-07  1e-14  3e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.4359e-01 -4.0740e+03  9e+03  1e-01  7e-11\n",
      " 1:  1.8594e-01 -1.2349e+02  2e+02  3e-03  5e-11\n",
      " 2:  3.0883e-02 -6.8676e+00  1e+01  2e-04  4e-11\n",
      " 3:  5.6412e-03 -5.1779e-01  9e-01  1e-05  6e-12\n",
      " 4:  1.2605e-03 -6.2107e-02  1e-01  1e-06  9e-13\n",
      " 5:  4.0611e-04 -1.0873e-02  2e-02  2e-07  2e-13\n",
      " 6:  1.6835e-04 -1.5493e-03  2e-03  2e-08  5e-14\n",
      " 7:  4.6857e-05 -1.0395e-04  2e-04  2e-10  2e-14\n",
      " 8: -2.3073e-06 -2.2146e-05  2e-05  2e-16  1e-14\n",
      " 9: -7.8441e-06 -1.2891e-05  5e-06  2e-16  6e-15\n",
      "10: -9.5043e-06 -1.1876e-05  2e-06  2e-16  5e-15\n",
      "11: -1.0478e-05 -1.0982e-05  5e-07  2e-16  6e-15\n",
      "12: -1.0674e-05 -1.0763e-05  9e-08  2e-16  6e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.3224e+03 -1.1988e+04  1e+04  1e-12  1e-15\n",
      " 1:  1.5040e+02 -1.1386e+03  1e+03  1e-12  1e-15\n",
      " 2: -1.6806e+02 -3.2829e+02  2e+02  8e-13  5e-16\n",
      " 3: -1.8537e+02 -1.9353e+02  8e+00  2e-13  2e-16\n",
      " 4: -1.8547e+02 -1.8556e+02  9e-02  9e-13  5e-17\n",
      " 5: -1.8547e+02 -1.8547e+02  9e-04  1e-12  7e-17\n",
      " 6: -1.8547e+02 -1.8547e+02  9e-06  9e-13  3e-17\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.4757e+05 -7.1299e+07  1e+08  2e-03  6e-08\n",
      " 1:  2.8429e+04 -3.8237e+06  7e+06  1e-04  5e-08\n",
      " 2:  4.9165e+03 -3.1271e+05  5e+05  7e-06  6e-09\n",
      " 3:  1.1620e+03 -4.0521e+04  7e+04  8e-07  9e-10\n",
      " 4:  3.6389e+02 -5.4230e+03  9e+03  9e-08  1e-10\n",
      " 5:  1.3880e+02 -5.6420e+02  9e+02  7e-09  3e-11\n",
      " 6:  3.2319e+01 -3.6832e+01  7e+01  6e-11  1e-11\n",
      " 7:  4.6874e+00 -5.1450e+00  1e+01  2e-16  6e-12\n",
      " 8:  6.7067e-01 -7.4092e-01  1e+00  2e-16  2e-12\n",
      " 9:  9.5697e-02 -1.0655e-01  2e-01  2e-16  1e-12\n",
      "10:  1.3557e-02 -1.5424e-02  3e-02  2e-16  4e-13\n",
      "11:  1.8814e-03 -2.2705e-03  4e-03  2e-16  1e-13\n",
      "12:  2.4399e-04 -3.4969e-04  6e-04  2e-16  6e-14\n",
      "13:  2.2440e-05 -6.1218e-05  8e-05  2e-16  2e-14\n",
      "14: -4.5498e-06 -1.7697e-05  1e-05  2e-16  9e-15\n",
      "15: -8.0184e-06 -1.2739e-05  5e-06  2e-16  6e-15\n",
      "16: -1.0009e-05 -1.1618e-05  2e-06  2e-16  5e-15\n",
      "17: -1.0526e-05 -1.0941e-05  4e-07  2e-16  5e-15\n",
      "18: -1.0684e-05 -1.0752e-05  7e-08  2e-16  6e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.2590e+09 -7.0404e+09  9e+09  2e-10  9e-13\n",
      " 1:  4.9259e+08 -5.5559e+08  1e+09  1e-09  2e-12\n",
      " 2:  7.1660e+07 -8.4698e+07  2e+08  2e-09  2e-13\n",
      " 3:  1.0307e+07 -1.1540e+07  2e+07  8e-10  2e-13\n",
      " 4:  1.4695e+06 -1.6597e+06  3e+06  1e-10  5e-14\n",
      " 5:  2.0674e+05 -2.4102e+05  4e+05  5e-11  1e-14\n",
      " 6:  2.8066e+04 -3.6042e+04  6e+04  1e-11  9e-15\n",
      " 7:  3.3682e+03 -5.7872e+03  9e+03  6e-12  4e-15\n",
      " 8:  1.7068e+02 -1.1120e+03  1e+03  6e-13  6e-16\n",
      " 9: -1.6732e+02 -3.2475e+02  2e+02  3e-12  3e-16\n",
      "10: -1.8536e+02 -1.9370e+02  8e+00  5e-13  2e-16\n",
      "11: -1.8547e+02 -1.8556e+02  9e-02  2e-13  8e-17\n",
      "12: -1.8547e+02 -1.8547e+02  9e-04  8e-13  4e-17\n",
      "13: -1.8547e+02 -1.8547e+02  9e-06  4e-13  5e-17\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.4757e+17 -6.7292e+19  1e+20  2e-03  7e-02\n",
      " 1:  2.7868e+16 -4.0192e+18  7e+18  1e-04  1e+01\n",
      " 2:  4.8814e+15 -3.2834e+17  5e+17  7e-06  7e-01\n",
      " 3:  1.1587e+15 -4.2683e+16  7e+16  8e-07  8e-02\n",
      " 4:  3.6378e+14 -5.6407e+15  9e+15  9e-08  1e-02\n",
      " 5:  1.3891e+14 -5.7419e+14  9e+14  7e-09  7e-04\n",
      " 6:  3.1980e+13 -3.6560e+13  7e+13  6e-11  1e-05\n",
      " 7:  4.6393e+12 -5.0808e+12  1e+13  4e-12  8e-06\n",
      " 8:  6.6485e+11 -7.3090e+11  1e+12  3e-12  3e-06\n",
      " 9:  9.5271e+10 -1.0470e+11  2e+11  6e-13  1e-06\n",
      "10:  1.3652e+10 -1.5004e+10  3e+10  6e-13  4e-07\n",
      "11:  1.9564e+09 -2.1500e+09  4e+09  1e-13  1e-07\n",
      "12:  2.8034e+08 -3.0810e+08  6e+08  3e-14  6e-08\n",
      "13:  4.0173e+07 -4.4150e+07  8e+07  4e-14  2e-08\n",
      "14:  5.7567e+06 -6.3266e+06  1e+07  2e-15  8e-09\n",
      "15:  8.2493e+05 -9.0660e+05  2e+06  3e-15  3e-09\n",
      "16:  1.1821e+05 -1.2991e+05  2e+05  7e-16  1e-09\n",
      "17:  1.6939e+04 -1.8617e+04  4e+04  5e-16  4e-10\n",
      "18:  2.4273e+03 -2.6678e+03  5e+03  2e-16  2e-10\n",
      "19:  3.4781e+02 -3.8232e+02  7e+02  2e-16  6e-11\n",
      "20:  4.9832e+01 -5.4795e+01  1e+02  2e-16  2e-11\n",
      "21:  7.1375e+00 -7.8553e+00  1e+01  2e-16  8e-12\n",
      "22:  1.0215e+00 -1.1269e+00  2e+00  2e-16  3e-12\n",
      "23:  1.4591e-01 -1.6196e-01  3e-01  1e-16  1e-12\n",
      "24:  2.0727e-02 -2.3390e-02  4e-02  2e-16  5e-13\n",
      "25:  2.8993e-03 -3.4215e-03  6e-03  1e-16  2e-13\n",
      "26:  3.8637e-04 -5.1816e-04  9e-04  2e-16  7e-14\n",
      "27:  4.1717e-05 -8.6534e-05  1e-04  2e-16  3e-14\n",
      "28: -1.8674e-06 -2.1334e-05  2e-05  2e-16  1e-14\n",
      "29: -7.3241e-06 -1.3222e-05  6e-06  2e-16  6e-15\n",
      "30: -9.1939e-06 -1.2329e-05  3e-06  2e-16  6e-15\n",
      "31: -1.0390e-05 -1.1160e-05  8e-07  7e-17  7e-15\n",
      "32: -1.0656e-05 -1.0794e-05  1e-07  8e-17  6e-15\n",
      "33: -1.0712e-05 -1.0718e-05  6e-09  2e-16  6e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.2598e+21 -7.0355e+21  9e+21  9e-04  1e-06\n",
      " 1:  4.9275e+20 -5.5510e+20  1e+21  2e-03  1e-06\n",
      " 2:  7.1755e+19 -8.4558e+19  2e+20  8e-04  2e-07\n",
      " 3:  1.0348e+19 -1.1493e+19  2e+19  2e-04  9e-08\n",
      " 4:  1.4858e+18 -1.6426e+18  3e+18  9e-05  6e-08\n",
      " 5:  2.1305e+17 -2.3461e+17  4e+17  4e-05  2e-08\n",
      " 6:  3.0538e+16 -3.3587e+16  6e+16  2e-05  4e-09\n",
      " 7:  4.3764e+15 -4.8110e+15  9e+15  8e-06  3e-09\n",
      " 8:  6.2715e+14 -6.8931e+14  1e+15  5e-06  8e-10\n",
      " 9:  8.9870e+13 -9.8771e+13  2e+14  5e-07  4e-10\n",
      "10:  1.2878e+13 -1.4153e+13  3e+13  4e-07  1e-10\n",
      "11:  1.8454e+12 -2.0281e+12  4e+12  1e-08  4e-11\n",
      "12:  2.6444e+11 -2.9063e+11  6e+11  3e-08  1e-11\n",
      "13:  3.7891e+10 -4.1648e+10  8e+10  2e-08  6e-12\n",
      "14:  5.4291e+09 -5.9687e+09  1e+10  6e-09  4e-12\n",
      "15:  7.7774e+08 -8.5553e+08  2e+09  7e-09  9e-13\n",
      "16:  1.1136e+08 -1.2268e+08  2e+08  1e-09  5e-13\n",
      "17:  1.5924e+07 -1.7614e+07  3e+07  5e-10  2e-13\n",
      "18:  2.2690e+06 -2.5369e+06  5e+06  4e-11  4e-14\n",
      "19:  3.2022e+05 -3.6842e+05  7e+05  6e-11  2e-14\n",
      "20:  4.3966e+04 -5.4685e+04  1e+05  9e-12  6e-15\n",
      "21:  5.5130e+03 -8.5937e+03  1e+04  7e-12  3e-15\n",
      "22:  4.2876e+02 -1.5634e+03  2e+03  5e-12  1e-15\n",
      "23: -1.4633e+02 -4.0433e+02  3e+02  1e-12  2e-16\n",
      "24: -1.8500e+02 -2.0397e+02  2e+01  2e-12  1e-16\n",
      "25: -1.8547e+02 -1.8572e+02  2e-01  2e-12  6e-17\n",
      "26: -1.8547e+02 -1.8547e+02  2e-03  5e-13  8e-17\n",
      "27: -1.8547e+02 -1.8547e+02  2e-05  2e-12  7e-17\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.4757e+27 -6.7292e+29  1e+30  1e-02  7e+03\n",
      " 1:  2.7868e+26 -4.0192e+28  7e+28  4e-03  1e+11\n",
      " 2:  4.8812e+25 -3.2830e+27  5e+27  4e-04  8e+09\n",
      " 3:  1.1587e+25 -4.2682e+26  7e+26  1e-04  9e+08\n",
      " 4:  3.6377e+24 -5.6405e+25  9e+25  2e-05  1e+08\n",
      " 5:  1.3891e+24 -5.7413e+24  9e+24  1e-06  8e+06\n",
      " 6:  3.1978e+23 -3.6557e+23  7e+23  1e-06  7e+04\n",
      " 7:  4.6389e+22 -5.0804e+22  1e+23  4e-07  7e-01\n",
      " 8:  6.6480e+21 -7.3084e+21  1e+22  5e-08  3e-01\n",
      " 9:  9.5264e+20 -1.0469e+21  2e+21  4e-08  1e-01\n",
      "10:  1.3651e+20 -1.5003e+20  3e+20  6e-08  4e-02\n",
      "11:  1.9562e+19 -2.1499e+19  4e+19  2e-08  1e-02\n",
      "12:  2.8032e+18 -3.0807e+18  6e+18  4e-09  5e-03\n",
      "13:  4.0170e+17 -4.4146e+17  8e+17  3e-10  2e-03\n",
      "14:  5.7563e+16 -6.3261e+16  1e+17  1e-09  7e-04\n",
      "15:  8.2487e+15 -9.0653e+15  2e+16  1e-10  3e-04\n",
      "16:  1.1820e+15 -1.2990e+15  2e+15  3e-10  1e-04\n",
      "17:  1.6938e+14 -1.8615e+14  4e+14  3e-11  4e-05\n",
      "18:  2.4272e+13 -2.6675e+13  5e+13  3e-11  2e-05\n",
      "19:  3.4782e+12 -3.8225e+12  7e+12  3e-15  6e-06\n",
      "20:  4.9842e+11 -5.4776e+11  1e+12  2e-12  2e-06\n",
      "21:  7.1423e+10 -7.8493e+10  1e+11  1e-12  9e-07\n",
      "22:  1.0235e+10 -1.1248e+10  2e+10  2e-13  3e-07\n",
      "23:  1.4666e+09 -1.6118e+09  3e+09  8e-14  1e-07\n",
      "24:  2.1016e+08 -2.3097e+08  4e+08  3e-14  5e-08\n",
      "25:  3.0116e+07 -3.3098e+07  6e+07  9e-15  2e-08\n",
      "26:  4.3156e+06 -4.7428e+06  9e+06  2e-15  6e-09\n",
      "27:  6.1842e+05 -6.7964e+05  1e+06  4e-15  2e-09\n",
      "28:  8.8618e+04 -9.7392e+04  2e+05  6e-16  9e-10\n",
      "29:  1.2699e+04 -1.3956e+04  3e+04  2e-16  4e-10\n",
      "30:  1.8196e+03 -1.9999e+03  4e+03  3e-16  1e-10\n",
      "31:  2.6073e+02 -2.8661e+02  5e+02  1e-16  5e-11\n",
      "32:  3.7355e+01 -4.1078e+01  8e+01  2e-16  2e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33:  5.3501e+00 -5.8891e+00  1e+01  1e-16  8e-12\n",
      "34:  7.6562e-01 -8.4495e-01  2e+00  4e-17  3e-12\n",
      "35:  1.0932e-01 -1.2148e-01  2e-01  7e-17  1e-12\n",
      "36:  1.5513e-02 -1.7558e-02  3e-02  1e-16  4e-13\n",
      "37:  2.1634e-03 -2.5745e-03  5e-03  2e-16  1e-13\n",
      "38:  2.8523e-04 -3.9248e-04  7e-04  2e-16  5e-14\n",
      "39:  2.8817e-05 -6.6931e-05  1e-04  1e-16  2e-14\n",
      "40: -2.2367e-06 -2.0356e-05  2e-05  1e-16  9e-15\n",
      "41: -7.2777e-06 -1.3152e-05  6e-06  2e-16  6e-15\n",
      "42: -9.2334e-06 -1.2409e-05  3e-06  2e-16  5e-15\n",
      "43: -1.0399e-05 -1.1132e-05  7e-07  2e-16  6e-15\n",
      "44: -1.0653e-05 -1.0798e-05  1e-07  7e-17  6e-15\n",
      "45: -1.0712e-05 -1.0718e-05  6e-09  1e-16  6e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.2598e+31 -7.0355e+31  9e+31  2e+01  9e-02\n",
      " 1:  4.9275e+30 -5.5510e+30  1e+31  2e+02  7e-02\n",
      " 2:  7.1755e+29 -8.4558e+29  2e+30  8e+01  3e-02\n",
      " 3:  1.0348e+29 -1.1493e+29  2e+29  7e+01  1e-02\n",
      " 4:  1.4858e+28 -1.6426e+28  3e+28  1e+01  5e-03\n",
      " 5:  2.1305e+27 -2.3461e+27  4e+27  7e+00  2e-03\n",
      " 6:  3.0538e+26 -3.3587e+26  6e+26  2e+00  4e-04\n",
      " 7:  4.3764e+25 -4.8110e+25  9e+25  3e-01  2e-04\n",
      " 8:  6.2715e+24 -6.8931e+24  1e+25  5e-01  1e-04\n",
      " 9:  8.9870e+23 -9.8771e+23  2e+24  6e-02  5e-05\n",
      "10:  1.2878e+23 -1.4153e+23  3e+23  1e-02  8e-06\n",
      "11:  1.8454e+22 -2.0281e+22  4e+22  1e-02  6e-06\n",
      "12:  2.6444e+21 -2.9062e+21  6e+21  1e-03  1e-06\n",
      "13:  3.7894e+20 -4.1645e+20  8e+20  5e-04  7e-07\n",
      "14:  5.4301e+19 -5.9677e+19  1e+20  2e-05  4e-07\n",
      "15:  7.7812e+18 -8.5515e+18  2e+19  2e-04  1e-07\n",
      "16:  1.1150e+18 -1.2254e+18  2e+18  3e-05  1e-07\n",
      "17:  1.5978e+17 -1.7560e+17  3e+17  6e-06  1e-08\n",
      "18:  2.2896e+16 -2.5163e+16  5e+16  3e-06  1e-08\n",
      "19:  3.2810e+15 -3.6058e+15  7e+15  7e-06  3e-09\n",
      "20:  4.7015e+14 -5.1670e+14  1e+15  7e-07  9e-10\n",
      "21:  6.7372e+13 -7.4042e+13  1e+14  2e-07  5e-10\n",
      "22:  9.6542e+12 -1.0610e+13  2e+13  6e-07  1e-10\n",
      "23:  1.3834e+12 -1.5204e+12  3e+12  2e-09  7e-11\n",
      "24:  1.9824e+11 -2.1787e+11  4e+11  9e-08  2e-11\n",
      "25:  2.8405e+10 -3.1222e+10  6e+10  2e-08  9e-12\n",
      "26:  4.0699e+09 -4.4746e+09  9e+09  4e-09  2e-12\n",
      "27:  5.8300e+08 -6.4140e+08  1e+09  6e-09  8e-13\n",
      "28:  8.3465e+07 -9.1988e+07  2e+08  5e-10  2e-13\n",
      "29:  1.1931e+07 -1.3211e+07  3e+07  9e-11  2e-13\n",
      "30:  1.6986e+06 -1.9042e+06  4e+06  1e-10  7e-14\n",
      "31:  2.3913e+05 -2.7710e+05  5e+05  1e-10  2e-14\n",
      "32:  3.2591e+04 -4.1354e+04  7e+04  4e-12  5e-15\n",
      "33:  3.9756e+03 -6.5909e+03  1e+04  4e-12  6e-15\n",
      "34:  2.4272e+02 -1.2422e+03  1e+03  5e-12  6e-16\n",
      "35: -1.6177e+02 -3.4778e+02  2e+02  8e-13  1e-16\n",
      "36: -1.8529e+02 -1.9645e+02  1e+01  2e-12  5e-17\n",
      "37: -1.8547e+02 -1.8560e+02  1e-01  2e-12  7e-17\n",
      "38: -1.8547e+02 -1.8547e+02  1e-03  3e-13  3e-17\n",
      "39: -1.8547e+02 -1.8547e+02  1e-05  1e-13  8e-17\n",
      "Optimal solution found.\n",
      "LIN SVM TIMES\n",
      "0.325848817825\n",
      "0.169378995895\n",
      "0.235991954803\n",
      "0.236642122269\n",
      "0.383164167404\n",
      "0.656485080719\n",
      "GAUSS SVM TIMES\n",
      "0.238135814667\n",
      "0.401993989944\n",
      "0.444653987885\n",
      "0.504105806351\n",
      "0.632975816727\n",
      "0.953361034393\n",
      "PEGASOS LIN SVM TIMES\n",
      "0.852132081985\n",
      "0.705545902252\n",
      "0.543320894241\n",
      "0.540467023849\n",
      "0.544455051422\n",
      "0.607100963593\n",
      "PEGASOS GAUSS SVM TIMES\n",
      "2.08829212189\n",
      "2.03218102455\n",
      "2.00071120262\n",
      "1.79623317719\n",
      "1.80458211899\n",
      "1.87130713463\n"
     ]
    }
   ],
   "source": [
    "##### 4.3\n",
    "\n",
    "import scipy\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform, cdist\n",
    "\n",
    "import time\n",
    "\n",
    "def shuffleData(n, X, Y):\n",
    "    indices = range(n)\n",
    "    shuffle(indices)\n",
    "    X = [X[i] for i in indices]\n",
    "    Y = [Y[i] for i in indices]\n",
    "    return (X,Y)\n",
    "\n",
    "def train(X,Y,n, features, lmbda, max_epochs):\n",
    "    t = 0\n",
    "    epoch = 0\n",
    "    w = np.zeros((features, 1))\n",
    "    b = np.zeros((1,1))\n",
    "    for epoch in xrange(max_epochs):\n",
    "        for i in xrange(n):\n",
    "            t += 1\n",
    "            eta = float(1)/(t*lmbda)\n",
    "            if np.dot(Y[i],np.dot(np.transpose(w), X[i]) + b) < 1:\n",
    "                w = np.add((1-eta*lmbda) * w,  eta * Y[i] * np.transpose(X[i][np.newaxis]))\n",
    "                b = np.add(b, (eta*Y[i]))\n",
    "            else:\n",
    "                w = (1-eta*lmbda) * w\n",
    "\n",
    "        X, Y = shuffleData(n, X, Y)\n",
    "    return (w,b)\n",
    "\n",
    "def computeKernel(X, gamma, n):\n",
    "    K = np.zeros((n,n));\n",
    "    for i in xrange(n):\n",
    "        for j in xrange(n):\n",
    "            K[i][j] = math.exp(- gamma * np.dot(np.transpose(X[i]-X[j]), X[i]-X[j]))\n",
    "    return K\n",
    "\n",
    "\n",
    "def train_gaussianSVM(X, Y, n, K, lmbda, max_epochs):\n",
    "    t = 0\n",
    "    epoch = 0\n",
    "    alpha = np.zeros((n,1)) \n",
    "    for epoch in xrange(max_epochs):\n",
    "        for i in xrange(n):\n",
    "            t += 1\n",
    "            eta = float(1)/(t*lmbda)\n",
    "            if np.dot(Y[i], np.dot(np.transpose(alpha),K[:,i])) < 1:\n",
    "                alpha[i] = (1-eta*lmbda)*alpha[i] + eta*Y[i]\n",
    "            else:\n",
    "                alpha[i] = (1-eta*lmbda)*alpha[i]\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def train(X,Y,n, features, lmbda, max_epochs):\n",
    "    t = 0\n",
    "    epoch = 0\n",
    "    w = np.zeros((features, 1))\n",
    "    b = np.zeros((1,1))\n",
    "    for epoch in xrange(max_epochs):\n",
    "        for i in xrange(n):\n",
    "            t += 1\n",
    "            eta = float(1)/(t*lmbda)\n",
    "            if np.dot(Y[i],np.dot(np.transpose(w), X[i]) + b) < 1:\n",
    "                w = np.add((1-eta*lmbda) * w,  eta * Y[i] * np.transpose(X[i][np.newaxis]))\n",
    "                b = np.add(b, (eta*Y[i]))\n",
    "            else:\n",
    "                w = (1-eta*lmbda) * w\n",
    "\n",
    "        X, Y = shuffleData(n, X, Y)\n",
    "    return (w,b)\n",
    "\n",
    "\n",
    "def calcTime(X, Y):\n",
    "\tX = X.T\n",
    "\tY = Y[np.newaxis]\n",
    "\tprint X.shape\n",
    "\tprint Y.shape\n",
    "\ttol = 1e-4\n",
    "\tgamma = 1\n",
    "\tsvmlin_times = []\n",
    "\tsvmgauss_times = []\n",
    "\tCs= [1e-5, 1e-2, 10, 1e4, 1e10, 1e15]\n",
    "\tfor i in xrange(len(Cs)):\n",
    "\t\tC = Cs[i]\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsvmlin(X,Y,C, tol)\n",
    "\t\ttime_elapsed = time.time() - start_time\n",
    "\t\tsvmlin_times.append(time_elapsed)\n",
    "\t\tstart_time_gauss = time.time()\n",
    "\t\tsvmgauss(X,Y,C, gamma, tol)\n",
    "\t\ttime_elapsed_gauss = time.time() - start_time_gauss\n",
    "\t\tsvmgauss_times.append(time_elapsed_gauss)\n",
    "\n",
    "\n",
    "\treturn svmlin_times, svmgauss_times\n",
    "\n",
    "def calcTimePegasos(X,Y, epochs):\n",
    "\n",
    "\tn_train = X.shape[0]\n",
    "\tfeatures_train = X.shape[1]\n",
    "\tgamma = 1\n",
    "\tsvmlin_times = []\n",
    "\tsvmgauss_times = []\n",
    "\tCs= [1e-5, 1e-2, 10, 1e4, 1e10, 1e15]\n",
    "\tlmbdas = [1 / x for x in Cs]\n",
    "        for i in xrange(len(Cs)):\n",
    "            lmbda = lmbdas[i]\n",
    "            start_time = time.time()\n",
    "            w,b = train(X, Y, n_train, features_train, lmbda, epochs)\n",
    "            time_elapsed = time.time() - start_time\n",
    "            svmlin_times.append(time_elapsed)\n",
    "            start_time_gauss = time.time()\n",
    "            K = computeKernel(X, gamma, n_train)\n",
    "            alpha = train_gaussianSVM(X, Y, n_train, K, lmbda, epochs)\n",
    "            time_elapsed_gauss = time.time() - start_time_gauss\n",
    "            svmgauss_times.append(time_elapsed_gauss)\n",
    "\n",
    "        return svmlin_times, svmgauss_times\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_epochs = 100\n",
    "time_elapsed, time_elapsed_gauss = calcTime(Xtrain1_7, Ytrain1_7)\n",
    "time_elapsed_peg, time_elapsed_gauss_peg = calcTimePegasos(Xtrain1_7, Ytrain1_7, max_epochs)\n",
    "\n",
    "print('LIN SVM TIMES')\n",
    "for time in time_elapsed: print time\n",
    "\n",
    "print('GAUSS SVM TIMES')\n",
    "for time in time_elapsed_gauss: print time\n",
    "\n",
    "print('PEGASOS LIN SVM TIMES')\n",
    "for time in time_elapsed_peg: print time\n",
    "\n",
    "print('PEGASOS GAUSS SVM TIMES')\n",
    "for time in time_elapsed_gauss_peg: print time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the questions below, you will first need to construct training, validation and test sets. You will do thatby collecting images for some subset of the digits, that will be labeled as +1, and some other subset that willbe labeled as -1.  Take the first 200 points for each digit and use them for training, use the next 150 pointsfor validation and the final 150 points as a test set.  If you are classifying (2,4) vs (3,5), then your trainingset will have 800 samples, your validation and training sets will have 600 samples each. Note that you willconstruct a vector of ones of length 400 and a vector of -1s, also of length 400, and these will be the labelsfor your training set, and so on.Be careful with the sizes of these data sets, you probably don’t wantto use all the samples in the data files.\n",
    "\n",
    "\n",
    "After constructing these data sets, you should also construct corresponding sets that are “normalized”, sothat each input feature (pixel) is in the range [-1, 1]. Because each feature in this dataset has minimum value0 and maximum value 255, you can do this normalization by transforming the wholeXmatrix (element-wise) to2X/255−1.Try (at least) the following pairwise classification tasks: 1 vs 7, 3 vs 5, 4 vs 9, (0, 2, 4, 6, 8) vs (1, 3, 5,7, 9).\n",
    "\n",
    "1.  Show the classification accuracy of your logistic regression from problem 1 and the linear SVM classifierfrom problem 2 on the MNIST data.  Compare and contrast the performance of each classifier (bothtraining  and  testing).   Does  normalization  of  the  data  matter?   Look  at  some  of  the  images  of  mis-classified digits, do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData(digit, ind1, ind2):\n",
    "    data = pl.loadtxt('/Users/becca/GitHub/6.867/code_hw2/data/mnist_digit_' + str(digit) + '.csv')\n",
    "    # Returns column matrices\n",
    "    X = data[ind1:ind2]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain1 = getData(1, 0, 200)\n",
    "Xtrain7 = getData(7, 0, 200)\n",
    "Xval1 = getData(1, 200, 350)\n",
    "Xval7 = getData(7, 200, 350)\n",
    "Xtest1 = getData(1, 350, 500)\n",
    "Xtest7 = getData(7, 350, 500)\n",
    "\n",
    "Xtrain3 = getData(3, 0, 200)\n",
    "Xtrain5 = getData(5, 0, 200)\n",
    "Xval3 = getData(3, 200, 350)\n",
    "Xval5 = getData(5, 200, 350)\n",
    "Xtest3 = getData(3, 350, 500)\n",
    "Xtest5 = getData(5, 350, 500)\n",
    "\n",
    "Xtrain4 = getData(4, 0, 200)\n",
    "Xtrain9 = getData(9, 0, 200)\n",
    "Xval4 = getData(4, 200, 350)\n",
    "Xval9 = getData(9, 200, 350)\n",
    "Xtest4 = getData(4, 350, 500)\n",
    "Xtest9 = getData(9, 350, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svmlin(X, Y, C, tol):\n",
    "    Xt = X.T\n",
    "    Yt = Y.T\n",
    "    # Data range\n",
    "    a = min(X[0])-0.5\n",
    "    b = max(X[0])+0.5\n",
    "    c = min(X[1])-0.5\n",
    "    d = max(X[1])+0.5\n",
    "    \n",
    "    # matrices for solver\n",
    "    A = matrix(Yt.T, tc = 'd')\n",
    "    b_mat = matrix(0, tc = 'd')\n",
    "    G = matrix(np.vstack(((-1*np.identity(len(Yt))), np.identity(len(Yt)))))\n",
    "    h = matrix(np.hstack((np.zeros(len(Yt)), np.repeat(C, len(Yt))))[np.newaxis].T)\n",
    "    q = matrix(np.repeat(-1, len(Yt))[np.newaxis].T, tc = 'd')\n",
    "    K = Yt*Xt\n",
    "    P = matrix(np.dot(K, K.T), tc = 'd')\n",
    "    # Call solver\n",
    "    sol = solvers.qp(P,q,G,h,A,b_mat)\n",
    "    alpha = np.asarray((sol['x']))\n",
    "    status = sol['status']\n",
    "    # Threshold alpha\n",
    "    tol = tol\n",
    "    alpha[alpha < tol] = 0\n",
    "    theta = np.dot(X, np.multiply(alpha, Yt))\n",
    "    \n",
    "    cond = (alpha > tol).reshape(-1)\n",
    "    SV = Xt[cond] #support vectors\n",
    "    bias_vec = Yt[cond] - np.dot(Xt[cond], theta)\n",
    "    test=np.histogram(bias_vec, bins=50000)\n",
    "    max(test[0])\n",
    "    bias = np.mean(test[1][test[0]==max(test[0])]) #most frequent bias\n",
    "    bias\n",
    "    return C, SV, theta, bias, a, b, c, d, bias_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.7328e-04 -4.0135e+02  2e+03  1e+00  1e-11\n",
      " 1:  3.8312e-03 -1.0575e+02  1e+02  2e-02  1e-11\n",
      " 2: -2.3430e-04 -6.0994e+00  7e+00  1e-03  1e-11\n",
      " 3: -1.1061e-04 -5.0113e-01  6e-01  8e-05  3e-12\n",
      " 4: -4.7118e-05 -6.5966e-02  7e-02  1e-05  7e-13\n",
      " 5: -5.2031e-06 -1.1844e-02  1e-02  2e-06  1e-13\n",
      " 6:  3.9680e-06 -2.7970e-03  3e-03  3e-07  4e-14\n",
      " 7:  1.0807e-06 -3.5293e-04  4e-04  4e-08  1e-14\n",
      " 8: -5.8270e-06 -3.1233e-05  3e-05  2e-09  7e-15\n",
      " 9: -8.2535e-06 -1.8264e-05  1e-05  7e-10  6e-15\n",
      "10: -9.5825e-06 -1.3204e-05  4e-06  2e-16  7e-15\n",
      "11: -1.0406e-05 -1.1288e-05  9e-07  2e-16  6e-15\n",
      "12: -1.0647e-05 -1.0812e-05  2e-07  2e-16  7e-15\n",
      "13: -1.0712e-05 -1.0718e-05  6e-09  2e-16  6e-15\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:32: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 50001 but corresponding boolean dimension is 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 400)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmlin(Xtrain1_7.T, Ytrain1_7[np.newaxis], 1, tol = 1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain02468 = np.reshape(np.array([getData(0, 0, 200), getData(2, 0, 200), getData(4, 0, 200), \n",
    "                        getData(6, 0, 200), getData(8, 0, 200)]), (1000, 784))\n",
    "Xtrain13579 = np.reshape(np.array([Xtrain1, Xtrain3, Xtrain5, Xtrain7, Xtrain9]), (1000, 784))\n",
    "\n",
    "Xval02468 = np.reshape(np.array([getData(0, 200, 350), getData(2, 200, 350), getData(4, 200, 350), \n",
    "                        getData(6, 200, 350), getData(8, 200, 350)]), (750, 784))\n",
    "Xval13579 = np.reshape(np.array([Xval1, Xval3, Xval5, Xval7, Xval9]), (750, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtest02468 = np.reshape(np.array([getData(0, 350, 500), getData(2, 350, 500), getData(4, 350, 500), \n",
    "                        getData(6, 350, 500), getData(8, 350, 500)]), (750, 784))\n",
    "Xtest13579 = np.reshape(np.array([Xtest1, Xtest3, Xtest5, Xtest7, Xtest9]), (750, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(train, val, test):\n",
    "    trainnorm = np.zeros((train.shape[0], train.shape[1]))\n",
    "    valnorm = np.zeros((val.shape[0], val.shape[1]))\n",
    "    testnorm = np.zeros((test.shape[0], test.shape[1]))\n",
    "\n",
    "    for i in xrange(train.shape[0]):\n",
    "        trainnorm[i] = 2*train[i]/255-1\n",
    "    for i in xrange(val.shape[0]):\n",
    "        valnorm[i] = 2*val[i]/255-1\n",
    "    for i in xrange(test.shape[0]):\n",
    "        testnorm[i] = 2*test[i]/255-1\n",
    "        \n",
    "    return trainnorm, valnorm, testnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain1norm, Xval1norm, Xtest1norm = normalize(Xtrain1, Xval1, Xtest1)\n",
    "Xtrain7norm, Xval7norm, Xtest7norm = normalize(Xtrain7, Xval7, Xtest7)\n",
    "\n",
    "Xtrain3norm, Xval3norm, Xtest3norm = normalize(Xtrain3, Xval3, Xtest3)\n",
    "Xtrain5norm, Xval5norm, Xtest5norm = normalize(Xtrain5, Xval5, Xtest5)\n",
    "\n",
    "Xtrain4norm, Xval4norm, Xtest4norm = normalize(Xtrain4, Xval4, Xtest4)\n",
    "Xtrain9norm, Xval9norm, Xtest9norm = normalize(Xtrain9, Xval9, Xtest9)\n",
    "\n",
    "Xtrain02468norm, Xval02468norm, Xtest02468norm = normalize(Xtrain02468, Xval02468, Xtest02468)\n",
    "Xtrain13579norm, Xval13579norm, Xtest13579norm = normalize(Xtrain13579, Xval13579, Xtest13579)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain1_7 = np.concatenate((Xtrain1, Xtrain7), axis = 0)\n",
    "Xtrain1_7norm = np.concatenate((Xtrain1norm, Xtrain7norm), axis = 0)\n",
    "\n",
    "Xval1_7 = np.concatenate((Xval1, Xval7), axis = 0)\n",
    "Xval1_7norm = np.concatenate((Xval1norm, Xval7norm), axis = 0)\n",
    "\n",
    "Xtest1_7 = np.concatenate((Xtest1, Xtest7), axis = 0)\n",
    "Xtest1_7norm = np.concatenate((Xtest1norm, Xtest7norm), axis = 0)\n",
    "\n",
    "Ytrain1_7 = np.concatenate((np.repeat(-1, len(Xtrain1)), np.repeat(1, len(Xtrain7))), axis = 0)\n",
    "Yval1_7 = np.concatenate((np.repeat(-1, len(Xval1)), np.repeat(1, len(Xval7))), axis = 0)\n",
    "Ytest1_7 = np.concatenate((np.repeat(-1, len(Xtest1)), np.repeat(1, len(Xtest7))), axis = 0)\n",
    "\n",
    "\n",
    "Xtrain3_5 = np.concatenate((Xtrain3, Xtrain5), axis = 0)\n",
    "Xtrain3_5norm = np.concatenate((Xtrain3norm, Xtrain5norm), axis = 0)\n",
    "\n",
    "Xval3_5 = np.concatenate((Xval3, Xval5), axis = 0)\n",
    "Xval3_5norm = np.concatenate((Xval3norm, Xval5norm), axis = 0)\n",
    "\n",
    "Xtest3_5 = np.concatenate((Xtest3, Xtest5), axis = 0)\n",
    "Xtest3_5norm = np.concatenate((Xtest3norm, Xtest5norm), axis = 0)\n",
    "\n",
    "Ytrain3_5 = np.concatenate((np.repeat(-1, len(Xtrain3)), np.repeat(1, len(Xtrain5))), axis = 0)\n",
    "Yval3_5 = np.concatenate((np.repeat(-1, len(Xval3)), np.repeat(1, len(Xval5))), axis = 0)\n",
    "Ytest3_5 = np.concatenate((np.repeat(-1, len(Xtest3)), np.repeat(1, len(Xtest5))), axis = 0)\n",
    "\n",
    "\n",
    "Xtrain4_9 = np.concatenate((Xtrain4, Xtrain9), axis = 0)\n",
    "Xtrain4_9norm = np.concatenate((Xtrain4norm, Xtrain9norm), axis = 0)\n",
    "\n",
    "Xval4_9 = np.concatenate((Xval4, Xval9), axis = 0)\n",
    "Xval4_9norm = np.concatenate((Xval4norm, Xval9norm), axis = 0)\n",
    "\n",
    "Xtest4_9 = np.concatenate((Xtest4, Xtest9), axis = 0)\n",
    "Xtest4_9norm = np.concatenate((Xtest4norm, Xtest9norm), axis = 0)\n",
    "\n",
    "Ytrain4_9 = np.concatenate((np.repeat(-1, len(Xtrain4)), np.repeat(1, len(Xtrain9))), axis = 0)\n",
    "Yval4_9 = np.concatenate((np.repeat(-1, len(Xval4)), np.repeat(1, len(Xval9))), axis = 0)\n",
    "Ytest4_9 = np.concatenate((np.repeat(-1, len(Xtest4)), np.repeat(1, len(Xtest9))), axis = 0)\n",
    "\n",
    "\n",
    "Xtrainmany = np.concatenate((Xtrain02468, Xtrain13579), axis = 0)\n",
    "Xtrainmanynorm = np.concatenate((Xtrain02468norm, Xtrain13579norm), axis = 0)\n",
    "\n",
    "Xvalmany = np.concatenate((Xval02468, Xval13579), axis = 0)\n",
    "Xvalmanynorm = np.concatenate((Xval02468norm, Xval13579norm), axis = 0)\n",
    "\n",
    "Xtestmany = np.concatenate((Xtest02468, Xtest13579), axis = 0)\n",
    "Xtestmanynorm = np.concatenate((Xtest02468norm, Xtest13579norm), axis = 0)\n",
    "\n",
    "Ytrainmany = np.concatenate((np.repeat(-1, len(Xtrain02468)), np.repeat(1, len(Xtrain13579))), axis = 0)\n",
    "Yvalmany = np.concatenate((np.repeat(-1, len(Xval02468)), np.repeat(1, len(Xval13579))), axis = 0)\n",
    "Ytestmany = np.concatenate((np.repeat(-1, len(Xtest02468)), np.repeat(1, len(Xtest13579))), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### done constructing datasets.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "x17valaccl2 = []\n",
    "x17testaccl2 = []\n",
    "x17valaccl2norm = []\n",
    "x17testaccl2norm = []\n",
    "for lamda in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "    clfl2 = sk.linear_model.LogisticRegression(penalty = 'l2', tol = 1e-10, C = 1/lamda, \n",
    "                                              fit_intercept = True, random_state = 10, \n",
    "                                         max_iter = 1000, intercept_scaling = 1e6)\n",
    "\n",
    "    clfl2.fit(Xtrain1_7, Ytrain1_7)\n",
    "    # Percent Agreement\n",
    "    #sum(clfl2.predict(Xtrain1_7) == Ytrain1_7)/len(Ytrain1_7)*100.0000 #100% for lamda 1000\n",
    "    x17valaccl2.append(sum(clfl2.predict(Xval1_7) == Yval1_7)/len(Yval1_7)*100.0000) #99% for lamda 1000\n",
    "    x17testaccl2.append(sum(clfl2.predict(Xtest1_7) == Ytest1_7)/len(Yval1_7)*100.0000) #98.67% for lamda 1000\n",
    "    \n",
    "    clfl2.fit(Xtrain1_7norm, Ytrain1_7)\n",
    "    x17valaccl2norm.append(sum(clfl2.predict(Xval1_7norm) == Yval1_7)/len(Yval1_7)*100.0000) \n",
    "    x17testaccl2norm.append(sum(clfl2.predict(Xtest1_7norm) == Ytest1_7)/len(Yval1_7)*100.0000) \n",
    "    \n",
    "\n",
    "x35valaccl2 = []\n",
    "x35testaccl2 = []\n",
    "x35valaccl2norm = []\n",
    "x35testaccl2norm = []\n",
    "for lamda in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "    clfl2 = sk.linear_model.LogisticRegression(penalty = 'l2', tol = 1e-10, C = 1/lamda, \n",
    "                                              fit_intercept = True, random_state = 10, \n",
    "                                         max_iter = 1000, intercept_scaling = 1e6)\n",
    "\n",
    "    clfl2.fit(Xtrain3_5, Ytrain3_5)\n",
    "    # Percent Agreement\n",
    "    #sum(clfl2.predict(Xtrain1_7) == Ytrain1_7)/len(Ytrain1_7)*100.0000 \n",
    "    x35valaccl2.append(sum(clfl2.predict(Xval3_5) == Yval3_5)/len(Yval3_5)*100.0000) \n",
    "    x35testaccl2.append(sum(clfl2.predict(Xtest3_5) == Ytest3_5)/len(Yval3_5)*100.0000) \n",
    "    \n",
    "    clfl2.fit(Xtrain3_5norm, Ytrain3_5)\n",
    "    x35valaccl2norm.append(sum(clfl2.predict(Xval3_5norm) == Yval3_5)/len(Yval3_5)*100.0000) \n",
    "    x35testaccl2norm.append(sum(clfl2.predict(Xtest3_5norm) == Ytest3_5)/len(Yval3_5)*100.0000) \n",
    "    \n",
    "    \n",
    "x49valaccl2 = []\n",
    "x49testaccl2 = []\n",
    "x49valaccl2norm = []\n",
    "x49testaccl2norm = []\n",
    "for lamda in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "    clfl2 = sk.linear_model.LogisticRegression(penalty = 'l2', tol = 1e-10, C = 1/lamda, \n",
    "                                              fit_intercept = True, random_state = 10, \n",
    "                                         max_iter = 1000, intercept_scaling = 1e6)\n",
    "\n",
    "    clfl2.fit(Xtrain4_9, Ytrain4_9)\n",
    "    # Percent Agreement\n",
    "    #sum(clfl2.predict(Xtrain1_7) == Ytrain1_7)/len(Ytrain1_7)*100.0000 \n",
    "    x49valaccl2.append(sum(clfl2.predict(Xval4_9) == Yval4_9)/len(Yval4_9)*100.0000) \n",
    "    x49testaccl2.append(sum(clfl2.predict(Xtest4_9) == Ytest4_9)/len(Yval4_9)*100.0000) \n",
    "    \n",
    "    clfl2.fit(Xtrain4_9norm, Ytrain4_9)\n",
    "    x49valaccl2norm.append(sum(clfl2.predict(Xval4_9norm) == Yval4_9)/len(Yval4_9)*100.0000) \n",
    "    x49testaccl2norm.append(sum(clfl2.predict(Xtest4_9norm) == Ytest4_9)/len(Yval4_9)*100.0000) \n",
    "    \n",
    "xmanyvalaccl2 = []\n",
    "xmanytestaccl2 = []\n",
    "xmanyvalaccl2norm = []\n",
    "xmanytestaccl2norm = []\n",
    "for lamda in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "    clfl2 = sk.linear_model.LogisticRegression(penalty = 'l2', tol = 1e-10, C = 1/lamda, \n",
    "                                              fit_intercept = True, random_state = 10, \n",
    "                                         max_iter = 1000, intercept_scaling = 1e6)\n",
    "\n",
    "    clfl2.fit(Xtrainmany, Ytrainmany)\n",
    "    # Percent Agreement\n",
    "    #sum(clfl2.predict(Xtrain1_7) == Ytrain1_7)/len(Ytrain1_7)*100.0000 \n",
    "    xmanyvalaccl2.append(sum(clfl2.predict(Xvalmany) == Yvalmany)/len(Yvalmany)*100.0000) \n",
    "    xmanytestaccl2.append(sum(clfl2.predict(Xtestmany) == Ytestmany)/len(Yvalmany)*100.0000) \n",
    "    \n",
    "    clfl2.fit(Xtrainmanynorm, Ytrainmany)\n",
    "    xmanyvalaccl2norm.append(sum(clfl2.predict(Xvalmanynorm) == Yvalmany)/len(Yvalmany)*100.0000) \n",
    "    xmanytestaccl2norm.append(sum(clfl2.predict(Xtestmanynorm) == Ytestmany)/len(Yvalmany)*100.0000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "xmanyvalaccsvm = []\n",
    "xmanytestaccsvm = []\n",
    "xmanyvalaccsvmnorm = []\n",
    "xmanytestaccsvmnorm = []\n",
    "for C in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "    model = svm.SVC(kernel='linear', C = C) \n",
    "    model.fit(Xtrainmany, np.ndarray.flatten(Ytrainmany))\n",
    "    xmanyvalaccsvm.append(sum(model.predict(Xvalmany) == Yvalmany)/len(Yvalmany)*100)\n",
    "    xmanytestaccsvm.append(sum(model.predict(Xtestmany) == Ytestmany)/len(Yvalmany)*100)\n",
    "    \n",
    "    model.fit(Xtrainmanynorm, np.ndarray.flatten(Ytrainmany))\n",
    "    xmanyvalaccsvmnorm.append(sum(model.predict(Xvalmanynorm) == Yvalmany)/len(Yvalmany)*100)\n",
    "    xmanytestaccsvmnorm.append(sum(model.predict(Xtestmanynorm) == Ytestmany)/len(Yvalmany)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "x17valaccsvm = []\n",
    "x17testaccsvm = []\n",
    "x17valaccsvmnorm = []\n",
    "x17testaccsvmnorm = []\n",
    "for C in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "    model = svm.SVC(kernel='linear', C = C) \n",
    "    model.fit(Xtrain1_7, np.ndarray.flatten(Ytrain1_7))\n",
    "    x17valaccsvm.append(sum(model.predict(Xval1_7) == Yval1_7)/len(Yval1_7)*100)\n",
    "    x17testaccsvm.append(sum(model.predict(Xtest1_7) == Ytest1_7)/len(Yval1_7)*100)\n",
    "\n",
    "    model.fit(Xtrain1_7norm, np.ndarray.flatten(Ytrain1_7))\n",
    "    x17valaccsvmnorm.append(sum(model.predict(Xval1_7norm) == Yval1_7)/len(Yval1_7)*100)\n",
    "    x17testaccsvmnorm.append(sum(model.predict(Xtest1_7norm) == Ytest1_7)/len(Yval1_7)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x35valaccsvm = []\n",
    "x35testaccsvm = []\n",
    "x35valaccsvmnorm = []\n",
    "x35testaccsvmnorm = []\n",
    "for C in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "    model = svm.SVC(kernel='linear', C = C) \n",
    "    model.fit(Xtrain3_5, np.ndarray.flatten(Ytrain3_5))\n",
    "    x35valaccsvm.append(sum(model.predict(Xval3_5) == Yval3_5)/len(Yval3_5)*100)\n",
    "    x35testaccsvm.append(sum(model.predict(Xtest3_5) == Ytest3_5)/len(Yval3_5)*100)\n",
    "\n",
    "    model.fit(Xtrain3_5norm, np.ndarray.flatten(Ytrain3_5))\n",
    "    x35valaccsvmnorm.append(sum(model.predict(Xval3_5norm) == Yval3_5)/len(Yval3_5)*100)\n",
    "    x35testaccsvmnorm.append(sum(model.predict(Xtest3_5norm) == Ytest3_5)/len(Yval3_5)*100)\n",
    "    \n",
    "x49valaccsvm = []\n",
    "x49testaccsvm = []\n",
    "x49valaccsvmnorm = []\n",
    "x49testaccsvmnorm = []\n",
    "for C in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "    model = svm.SVC(kernel='linear', C = C) \n",
    "    model.fit(Xtrain4_9, np.ndarray.flatten(Ytrain4_9))\n",
    "    x49valaccsvm.append(sum(model.predict(Xval4_9) == Yval4_9)/len(Yval4_9)*100)\n",
    "    x49testaccsvm.append(sum(model.predict(Xtest4_9) == Ytest4_9)/len(Yval4_9)*100)\n",
    "\n",
    "    model.fit(Xtrain4_9norm, np.ndarray.flatten(Ytrain4_9))\n",
    "    x49valaccsvmnorm.append(sum(model.predict(Xval4_9norm) == Yval4_9)/len(Yval4_9)*100)\n",
    "    x49testaccsvmnorm.append(sum(model.predict(Xtest4_9norm) == Ytest4_9)/len(Yval4_9)*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n"
     ]
    }
   ],
   "source": [
    "# Gauss kernel\n",
    "from sklearn import svm\n",
    "\n",
    "xmanyvalaccsvmg = []\n",
    "xmanytestaccsvmg = []\n",
    "xmanyvalaccsvmnormg = []\n",
    "xmanytestaccsvmnormg = []\n",
    "for C in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "    for gamma in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "        model = svm.SVC(kernel='rbf', C = C, gamma = gamma) \n",
    "        model.fit(Xtrainmany, np.ndarray.flatten(Ytrainmany))\n",
    "        xmanyvalaccsvmg.append(sum(model.predict(Xvalmany) == Yvalmany)/len(Yvalmany)*100)\n",
    "        xmanytestaccsvmg.append(sum(model.predict(Xtestmany) == Ytestmany)/len(Yvalmany)*100)\n",
    "\n",
    "        model.fit(Xtrainmanynorm, np.ndarray.flatten(Ytrainmany))\n",
    "        xmanyvalaccsvmnormg.append(sum(model.predict(Xvalmanynorm) == Yvalmany)/len(Yvalmany)*100)\n",
    "        xmanytestaccsvmnormg.append(sum(model.predict(Xtestmanynorm) == Ytestmany)/len(Yvalmany)*100)\n",
    "        print gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n",
      "1e-05\n",
      "0.01\n",
      "10\n",
      "10000.0\n",
      "10000000000.0\n",
      "1e+30\n"
     ]
    }
   ],
   "source": [
    "# more gauss kernel\n",
    "x17valaccsvmg = []\n",
    "x17testaccsvmg = []\n",
    "x17valaccsvmnormg = []\n",
    "x17testaccsvmnormg = []\n",
    "for C in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "    for gamma in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "        model = svm.SVC(kernel='rbf', C = C, gamma = gamma) \n",
    "        model.fit(Xtrain1_7, np.ndarray.flatten(Ytrain1_7))\n",
    "        x17valaccsvmg.append(sum(model.predict(Xval1_7) == Yval1_7)/len(Yval1_7)*100)\n",
    "        x17testaccsvmg.append(sum(model.predict(Xtest1_7) == Ytest1_7)/len(Yval1_7)*100)\n",
    "\n",
    "        model.fit(Xtrain1_7norm, np.ndarray.flatten(Ytrain1_7))\n",
    "        x17valaccsvmnormg.append(sum(model.predict(Xval1_7norm) == Yval1_7)/len(Yval1_7)*100)\n",
    "        x17testaccsvmnormg.append(sum(model.predict(Xtest1_7norm) == Ytest1_7)/len(Yval1_7)*100)\n",
    "        print gamma\n",
    "\n",
    "\n",
    "x35valaccsvmg = []\n",
    "x35testaccsvmg = []\n",
    "x35valaccsvmnormg = []\n",
    "x35testaccsvmnormg = []\n",
    "for C in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "    for gamma in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "        model = svm.SVC(kernel='rbf', C = C, gamma = gamma) \n",
    "        model.fit(Xtrain3_5, np.ndarray.flatten(Ytrain3_5))\n",
    "        x35valaccsvmg.append(sum(model.predict(Xval3_5) == Yval3_5)/len(Yval3_5)*100)\n",
    "        x35testaccsvmg.append(sum(model.predict(Xtest3_5) == Ytest3_5)/len(Yval3_5)*100)\n",
    "\n",
    "        model.fit(Xtrain3_5norm, np.ndarray.flatten(Ytrain3_5))\n",
    "        x35valaccsvmnormg.append(sum(model.predict(Xval3_5norm) == Yval3_5)/len(Yval3_5)*100)\n",
    "        x35testaccsvmnormg.append(sum(model.predict(Xtest3_5norm) == Ytest3_5)/len(Yval3_5)*100)\n",
    "        print gamma\n",
    "    \n",
    "x49valaccsvmg = []\n",
    "x49testaccsvmg = []\n",
    "x49valaccsvmnormg = []\n",
    "x49testaccsvmnormg = []\n",
    "for C in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "    for gamma in [1e-5, 1e-2, 10, 1e4, 1e10, 1e30]:\n",
    "        model = svm.SVC(kernel='rbf', C = C, gamma = gamma) \n",
    "        model.fit(Xtrain4_9, np.ndarray.flatten(Ytrain4_9))\n",
    "        x49valaccsvmg.append(sum(model.predict(Xval4_9) == Yval4_9)/len(Yval4_9)*100)\n",
    "        x49testaccsvmg.append(sum(model.predict(Xtest4_9) == Ytest4_9)/len(Yval4_9)*100)\n",
    "\n",
    "        model.fit(Xtrain4_9norm, np.ndarray.flatten(Ytrain4_9))\n",
    "        x49valaccsvmnormg.append(sum(model.predict(Xval4_9norm) == Yval4_9)/len(Yval4_9)*100)\n",
    "        x49testaccsvmnormg.append(sum(model.predict(Xtest4_9norm) == Ytest4_9)/len(Yval4_9)*100)\n",
    "\n",
    "        print gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick test acc corr to max val accuracy (if tie, picked min C)\n",
    "\n",
    "svmg = np.array([x17testaccsvmg[30], \n",
    "               x17testaccsvmnormg[31], \n",
    "               x35testaccsvmg[6],\n",
    "               x35testaccsvmnormg[31],\n",
    "               x49testaccsvmg[6],\n",
    "               x49testaccsvmnormg[31],\n",
    "               xmanytestaccsvmg[6],\n",
    "               xmanytestaccsvmnormg[31]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sklearn.svm' from '/anaconda/lib/python2.7/site-packages/sklearn/svm/__init__.pyc'>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick test acc corr to max val accuracy (if tie, picked min C, max lambda)\n",
    "l2 = np.array([x17testaccl2[3], \n",
    "               x17testaccl2norm[2], \n",
    "               x35testaccl2[3],\n",
    "               x35testaccl2norm[1],\n",
    "               x49testaccl2[x49valaccl2.index(max(x49valaccl2))],\n",
    "               x49testaccl2norm[x49valaccl2norm.index(max(x49valaccl2norm))],\n",
    "               xmanytestaccl2[xmanyvalaccl2.index(max(xmanyvalaccl2))],\n",
    "               xmanytestaccl2norm[xmanyvalaccl2norm.index(max(xmanyvalaccl2norm))]\n",
    "              ])\n",
    "\n",
    "svmpd = np.array([x17testaccsvm[0], \n",
    "               x17testaccsvmnorm[1], \n",
    "               x35testaccsvm[0],\n",
    "               x35testaccsvmnorm[x35valaccsvmnorm.index(max(x35valaccsvmnorm))],\n",
    "               x49testaccsvm[x49valaccsvm.index(max(x49valaccsvm))],\n",
    "               x49testaccsvmnorm[x49valaccsvmnorm.index(max(x49valaccsvmnorm))],\n",
    "               xmanytestaccsvm[xmanyvalaccsvm.index(max(xmanyvalaccsvm))],\n",
    "               xmanytestaccsvmnorm[xmanyvalaccsvmnorm.index(max(xmanyvalaccsvmnorm))]\n",
    "              ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "l2svmacc = pd.DataFrame(data = np.array([l2, svmpd]).T, columns = ['L2', 'Linear SVM'])\n",
    "l2svmacc.index = ['1 vs 7', '1 vs 7 norm', '3 vs 5', '3 vs 5 norm', '4 vs 9', '4 vs 9 norm', '02468 vs 13579', '02468 vs 13579 norm']\n",
    "\n",
    "svmacc =  pd.DataFrame(data = np.array([svmpd, svmg]).T, columns = ['Linear SVM', 'Gaussian RBF'])\n",
    "svmacc.index = ['1 vs 7', '1 vs 7 norm', '3 vs 5', '3 vs 5 norm', '4 vs 9', '4 vs 9 norm', '02468 vs 13579', '02468 vs 13579 norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimum RBF gamma C:\n",
    "17 : gamma = 1e-5, C = 1e30\n",
    "17norm: gamma = 1e-2, C = 1e30\n",
    "35: gamma = 1e-5, C = 1e-2\n",
    "35 norm: gamma = 1e-2, C = 1e30\n",
    "49: gamma = 1e-5, C = 1e-2\n",
    "49 norm: gamma = 1e-2, C = 1e30\n",
    "many norm: gamma = 1e-5, C = 1e-2\n",
    "many norm: gamma = 1e-2, C = 1e30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+10</td>\n",
       "      <td>1.000000e+10</td>\n",
       "      <td>1.000000e+10</td>\n",
       "      <td>1.000000e+10</td>\n",
       "      <td>1.000000e+30</td>\n",
       "      <td>1.000000e+30</td>\n",
       "      <td>1.000000e+30</td>\n",
       "      <td>1.000000e+30</td>\n",
       "      <td>1.000000e+30</td>\n",
       "      <td>1.000000e+30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.000000e+10</td>\n",
       "      <td>1.000000e+30</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>1.000000e+10</td>\n",
       "      <td>1.000000e+30</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>1.000000e+10</td>\n",
       "      <td>1.000000e+30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1         2            3             4             5   \\\n",
       "0  0.00001  0.00001   0.00001      0.00001  1.000000e-05  1.000000e-05   \n",
       "1  0.00001  0.01000  10.00000  10000.00000  1.000000e+10  1.000000e+30   \n",
       "\n",
       "        6     7      8         9       ...                 26            27  \\\n",
       "0  0.01000  0.01   0.01      0.01      ...       1.000000e+10  1.000000e+10   \n",
       "1  0.00001  0.01  10.00  10000.00      ...       1.000000e+01  1.000000e+04   \n",
       "\n",
       "             28            29            30            31            32  \\\n",
       "0  1.000000e+10  1.000000e+10  1.000000e+30  1.000000e+30  1.000000e+30   \n",
       "1  1.000000e+10  1.000000e+30  1.000000e-05  1.000000e-02  1.000000e+01   \n",
       "\n",
       "             33            34            35  \n",
       "0  1.000000e+30  1.000000e+30  1.000000e+30  \n",
       "1  1.000000e+04  1.000000e+10  1.000000e+30  \n",
       "\n",
       "[2 rows x 36 columns]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2 = np.repeat([1e-5, 1e-2, 10, 1e4, 1e10, 1e30], 6).T\n",
    "gamma2 = np.tile([1e-5, 1e-2, 10, 1e4, 1e10, 1e30], 6).T\n",
    "xmanytestaccsvmnormg\n",
    "pd.DataFrame([C2, gamma2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick test acc corr to max val accuracy (if tie, picked min C)\n",
    "\n",
    "svmg = np.array([x17testaccsvmg[30], \n",
    "               x17testaccsvmnormg[31], \n",
    "               x35testaccsvmg[6],\n",
    "               x35testaccsvmnormg[31],\n",
    "               x49testaccsvmg[6],\n",
    "               x49testaccsvmnormg[31],\n",
    "               xmanytestaccsvmg[6],\n",
    "               xmanytestaccsvmnormg[31]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear SVM</th>\n",
       "      <th>Gaussian RBF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1 vs 7</th>\n",
       "      <td>98.666667</td>\n",
       "      <td>82.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 vs 7 norm</th>\n",
       "      <td>98.666667</td>\n",
       "      <td>99.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 vs 5</th>\n",
       "      <td>94.666667</td>\n",
       "      <td>65.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 vs 5 norm</th>\n",
       "      <td>95.333333</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 vs 9</th>\n",
       "      <td>94.333333</td>\n",
       "      <td>88.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 vs 9 norm</th>\n",
       "      <td>93.666667</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02468 vs 13579</th>\n",
       "      <td>89.133333</td>\n",
       "      <td>76.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02468 vs 13579 norm</th>\n",
       "      <td>88.666667</td>\n",
       "      <td>97.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Linear SVM  Gaussian RBF\n",
       "1 vs 7                98.666667     82.666667\n",
       "1 vs 7 norm           98.666667     99.333333\n",
       "3 vs 5                94.666667     65.333333\n",
       "3 vs 5 norm           95.333333     99.000000\n",
       "4 vs 9                94.333333     88.333333\n",
       "4 vs 9 norm           93.666667     98.000000\n",
       "02468 vs 13579        89.133333     76.200000\n",
       "02468 vs 13579 norm   88.666667     97.400000"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot SVM and L2 regression max test accuracy results\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.6)\n",
    "f,(ax1) = plt.subplots(1,1,sharey=True, figsize = (4,5))\n",
    "g1 = sns.heatmap(l2svmacc, cmap = inferno, ax = ax1, annot = True, annot_kws={\"size\": 16})\n",
    "plt.yticks(rotation=0) \n",
    "#ax1.set_title('Test Accuracy ')\n",
    "\n",
    "plt.xlabel('')\n",
    "\n",
    "plt.savefig('L2 vs linear SVM test acc.jpg', dpi= 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot SVM and SVM gauss max test accuracy results\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.6)\n",
    "f,(ax1) = plt.subplots(1,1,sharey=True, figsize = (5,5))\n",
    "g1 = sns.heatmap(svmacc, cmap = inferno, ax = ax1, annot = True, annot_kws={\"size\": 16})\n",
    "plt.yticks(rotation=0) \n",
    "#ax1.set_title('Test Accuracy ')\n",
    "\n",
    "plt.xlabel('')\n",
    "\n",
    "plt.savefig('linear SVM vs gauss rbf test acc.jpg', dpi= 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot SVM and L2 regression max test accuracy results for Emily\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.6)\n",
    "f,(ax1) = plt.subplots(1,1,sharey=True, figsize = (4,5))\n",
    "g1 = sns.heatmap(l2svmacc, cmap = viridis, ax = ax1, annot = True, annot_kws={\"size\": 16})\n",
    "plt.yticks(rotation=0) \n",
    "#ax1.set_title('Test Accuracy ')\n",
    "\n",
    "plt.xlabel('')\n",
    "\n",
    "plt.savefig('L2 vs linear SVM test acc for Emily.jpg', dpi= 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SVM and SVM gauss max test accuracy results\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.6)\n",
    "f,(ax1) = plt.subplots(1,1,sharey=True, figsize = (5,5))\n",
    "g1 = sns.heatmap(svmacc, cmap = viridis, ax = ax1, annot = True, annot_kws={\"size\": 16})\n",
    "plt.yticks(rotation=0) \n",
    "#ax1.set_title('Test Accuracy ')\n",
    "\n",
    "plt.xlabel('')\n",
    "\n",
    "plt.savefig('linear SVM vs gauss rbf test acc for Emily.jpg', dpi= 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.0"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "clfl2.decision_function(Xtrain1_7).shape\n",
    "# Percent Agreement\n",
    "sum(clfl2.predict(Xtrain1_7) == Ytrain1_7)/len(Ytrain1_7)*100.0000 #100%\n",
    "sum(clfl2.predict(Xval1_7) == Yval1_7)/len(Yval1_7)*100.0000 #99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamda = 1e3\n",
    "clfl2 = sk.linear_model.LogisticRegression(penalty = 'l2', tol = 1e-10, C = 1/lamda, \n",
    "                                              fit_intercept = True, random_state = 10, \n",
    "                                         max_iter = 1000, intercept_scaling = 1e6)\n",
    "clfl2.fit(Xtrain1_7, Ytrain1_7)\n",
    "(clfl2.predict(Xval1_7) == Yval1_7)[129] # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot image of misclassified digit..makes sense\n",
    "index = 129\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "\n",
    "a =np.array(Xval1[index]).reshape((28, 28))\n",
    "plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "fig.savefig('l2 lambda = 1000 misclassif 1.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr classif\n",
    "(clfl2.predict(Xval1_7) == Yval1_7)[1] # true\n",
    "index = 1\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "\n",
    "a =np.array(Xval1[index]).reshape((28, 28))\n",
    "plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "fig.savefig('l2 lambda = 1000 corr classif 1.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr classif\n",
    "(clfl2.predict(Xval1_7) == Yval1_7)[151] # true\n",
    "index = 151-150\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "\n",
    "a =np.array(Xval7[index]).reshape((28, 28))\n",
    "plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "fig.savefig('l2 lambda = 1000 corr classif 7.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "(clfl2.predict(Xval1_7) == Yval1_7)[227] # False\n",
    "# Plot image of misclassified digit..makes sense\n",
    "index = 227-150\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "\n",
    "a =np.array(Xval7[index]).reshape((28, 28))\n",
    "plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "fig.savefig('l2 lambda = 1000 misclassif 7.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamda = 1e3\n",
    "clfl2 = sk.linear_model.LogisticRegression(penalty = 'l2', tol = 1e-10, C = 1/lamda, \n",
    "                                              fit_intercept = True, random_state = 10, \n",
    "                                         max_iter = 1000, intercept_scaling = 1e6)\n",
    "clfl2.fit(Xtrain3_5, Ytrain3_5)\n",
    "(clfl2.predict(Xval3_5) == Yval3_5)[235] # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot image of misclassified digits..makes sense - for Emily\n",
    "index = 30\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "a =np.array(Xval3[index]).reshape((28, 28))\n",
    "plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "fig.savefig('l2 lambda = 1000 misclassif 3.jpg', bbox_inches='tight')\n",
    "\n",
    "index = 1\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "a =np.array(Xval3[index]).reshape((28, 28))\n",
    "plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "fig.savefig('l2 lambda = 1000 corr classif 3.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot image of misclassified digits..makes sense - for Emily\n",
    "index = 235-150\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "a =np.array(Xval5[index]).reshape((28, 28))\n",
    "plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "fig.savefig('l2 lambda = 1000 misclassif 5.jpg', bbox_inches='tight')\n",
    "\n",
    "index = 237-150\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "a =np.array(Xval5[index]).reshape((28, 28))\n",
    "plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "fig.savefig('l2 lambda = 1000 corr classif 5.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_inferno_data = [[0.001462, 0.000466, 0.013866],\n",
    "                 [0.002267, 0.001270, 0.018570],\n",
    "                 [0.003299, 0.002249, 0.024239],\n",
    "                 [0.004547, 0.003392, 0.030909],\n",
    "                 [0.006006, 0.004692, 0.038558],\n",
    "                 [0.007676, 0.006136, 0.046836],\n",
    "                 [0.009561, 0.007713, 0.055143],\n",
    "                 [0.011663, 0.009417, 0.063460],\n",
    "                 [0.013995, 0.011225, 0.071862],\n",
    "                 [0.016561, 0.013136, 0.080282],\n",
    "                 [0.019373, 0.015133, 0.088767],\n",
    "                 [0.022447, 0.017199, 0.097327],\n",
    "                 [0.025793, 0.019331, 0.105930],\n",
    "                 [0.029432, 0.021503, 0.114621],\n",
    "                 [0.033385, 0.023702, 0.123397],\n",
    "                 [0.037668, 0.025921, 0.132232],\n",
    "                 [0.042253, 0.028139, 0.141141],\n",
    "                 [0.046915, 0.030324, 0.150164],\n",
    "                 [0.051644, 0.032474, 0.159254],\n",
    "                 [0.056449, 0.034569, 0.168414],\n",
    "                 [0.061340, 0.036590, 0.177642],\n",
    "                 [0.066331, 0.038504, 0.186962],\n",
    "                 [0.071429, 0.040294, 0.196354],\n",
    "                 [0.076637, 0.041905, 0.205799],\n",
    "                 [0.081962, 0.043328, 0.215289],\n",
    "                 [0.087411, 0.044556, 0.224813],\n",
    "                 [0.092990, 0.045583, 0.234358],\n",
    "                 [0.098702, 0.046402, 0.243904],\n",
    "                 [0.104551, 0.047008, 0.253430],\n",
    "                 [0.110536, 0.047399, 0.262912],\n",
    "                 [0.116656, 0.047574, 0.272321],\n",
    "                 [0.122908, 0.047536, 0.281624],\n",
    "                 [0.129285, 0.047293, 0.290788],\n",
    "                 [0.135778, 0.046856, 0.299776],\n",
    "                 [0.142378, 0.046242, 0.308553],\n",
    "                 [0.149073, 0.045468, 0.317085],\n",
    "                 [0.155850, 0.044559, 0.325338],\n",
    "                 [0.162689, 0.043554, 0.333277],\n",
    "                 [0.169575, 0.042489, 0.340874],\n",
    "                 [0.176493, 0.041402, 0.348111],\n",
    "                 [0.183429, 0.040329, 0.354971],\n",
    "                 [0.190367, 0.039309, 0.361447],\n",
    "                 [0.197297, 0.038400, 0.367535],\n",
    "                 [0.204209, 0.037632, 0.373238],\n",
    "                 [0.211095, 0.037030, 0.378563],\n",
    "                 [0.217949, 0.036615, 0.383522],\n",
    "                 [0.224763, 0.036405, 0.388129],\n",
    "                 [0.231538, 0.036405, 0.392400],\n",
    "                 [0.238273, 0.036621, 0.396353],\n",
    "                 [0.244967, 0.037055, 0.400007],\n",
    "                 [0.251620, 0.037705, 0.403378],\n",
    "                 [0.258234, 0.038571, 0.406485],\n",
    "                 [0.264810, 0.039647, 0.409345],\n",
    "                 [0.271347, 0.040922, 0.411976],\n",
    "                 [0.277850, 0.042353, 0.414392],\n",
    "                 [0.284321, 0.043933, 0.416608],\n",
    "                 [0.290763, 0.045644, 0.418637],\n",
    "                 [0.297178, 0.047470, 0.420491],\n",
    "                 [0.303568, 0.049396, 0.422182],\n",
    "                 [0.309935, 0.051407, 0.423721],\n",
    "                 [0.316282, 0.053490, 0.425116],\n",
    "                 [0.322610, 0.055634, 0.426377],\n",
    "                 [0.328921, 0.057827, 0.427511],\n",
    "                 [0.335217, 0.060060, 0.428524],\n",
    "                 [0.341500, 0.062325, 0.429425],\n",
    "                 [0.347771, 0.064616, 0.430217],\n",
    "                 [0.354032, 0.066925, 0.430906],\n",
    "                 [0.360284, 0.069247, 0.431497],\n",
    "                 [0.366529, 0.071579, 0.431994],\n",
    "                 [0.372768, 0.073915, 0.432400],\n",
    "                 [0.379001, 0.076253, 0.432719],\n",
    "                 [0.385228, 0.078591, 0.432955],\n",
    "                 [0.391453, 0.080927, 0.433109],\n",
    "                 [0.397674, 0.083257, 0.433183],\n",
    "                 [0.403894, 0.085580, 0.433179],\n",
    "                 [0.410113, 0.087896, 0.433098],\n",
    "                 [0.416331, 0.090203, 0.432943],\n",
    "                 [0.422549, 0.092501, 0.432714],\n",
    "                 [0.428768, 0.094790, 0.432412],\n",
    "                 [0.434987, 0.097069, 0.432039],\n",
    "                 [0.441207, 0.099338, 0.431594],\n",
    "                 [0.447428, 0.101597, 0.431080],\n",
    "                 [0.453651, 0.103848, 0.430498],\n",
    "                 [0.459875, 0.106089, 0.429846],\n",
    "                 [0.466100, 0.108322, 0.429125],\n",
    "                 [0.472328, 0.110547, 0.428334],\n",
    "                 [0.478558, 0.112764, 0.427475],\n",
    "                 [0.484789, 0.114974, 0.426548],\n",
    "                 [0.491022, 0.117179, 0.425552],\n",
    "                 [0.497257, 0.119379, 0.424488],\n",
    "                 [0.503493, 0.121575, 0.423356],\n",
    "                 [0.509730, 0.123769, 0.422156],\n",
    "                 [0.515967, 0.125960, 0.420887],\n",
    "                 [0.522206, 0.128150, 0.419549],\n",
    "                 [0.528444, 0.130341, 0.418142],\n",
    "                 [0.534683, 0.132534, 0.416667],\n",
    "                 [0.540920, 0.134729, 0.415123],\n",
    "                 [0.547157, 0.136929, 0.413511],\n",
    "                 [0.553392, 0.139134, 0.411829],\n",
    "                 [0.559624, 0.141346, 0.410078],\n",
    "                 [0.565854, 0.143567, 0.408258],\n",
    "                 [0.572081, 0.145797, 0.406369],\n",
    "                 [0.578304, 0.148039, 0.404411],\n",
    "                 [0.584521, 0.150294, 0.402385],\n",
    "                 [0.590734, 0.152563, 0.400290],\n",
    "                 [0.596940, 0.154848, 0.398125],\n",
    "                 [0.603139, 0.157151, 0.395891],\n",
    "                 [0.609330, 0.159474, 0.393589],\n",
    "                 [0.615513, 0.161817, 0.391219],\n",
    "                 [0.621685, 0.164184, 0.388781],\n",
    "                 [0.627847, 0.166575, 0.386276],\n",
    "                 [0.633998, 0.168992, 0.383704],\n",
    "                 [0.640135, 0.171438, 0.381065],\n",
    "                 [0.646260, 0.173914, 0.378359],\n",
    "                 [0.652369, 0.176421, 0.375586],\n",
    "                 [0.658463, 0.178962, 0.372748],\n",
    "                 [0.664540, 0.181539, 0.369846],\n",
    "                 [0.670599, 0.184153, 0.366879],\n",
    "                 [0.676638, 0.186807, 0.363849],\n",
    "                 [0.682656, 0.189501, 0.360757],\n",
    "                 [0.688653, 0.192239, 0.357603],\n",
    "                 [0.694627, 0.195021, 0.354388],\n",
    "                 [0.700576, 0.197851, 0.351113],\n",
    "                 [0.706500, 0.200728, 0.347777],\n",
    "                 [0.712396, 0.203656, 0.344383],\n",
    "                 [0.718264, 0.206636, 0.340931],\n",
    "                 [0.724103, 0.209670, 0.337424],\n",
    "                 [0.729909, 0.212759, 0.333861],\n",
    "                 [0.735683, 0.215906, 0.330245],\n",
    "                 [0.741423, 0.219112, 0.326576],\n",
    "                 [0.747127, 0.222378, 0.322856],\n",
    "                 [0.752794, 0.225706, 0.319085],\n",
    "                 [0.758422, 0.229097, 0.315266],\n",
    "                 [0.764010, 0.232554, 0.311399],\n",
    "                 [0.769556, 0.236077, 0.307485],\n",
    "                 [0.775059, 0.239667, 0.303526],\n",
    "                 [0.780517, 0.243327, 0.299523],\n",
    "                 [0.785929, 0.247056, 0.295477],\n",
    "                 [0.791293, 0.250856, 0.291390],\n",
    "                 [0.796607, 0.254728, 0.287264],\n",
    "                 [0.801871, 0.258674, 0.283099],\n",
    "                 [0.807082, 0.262692, 0.278898],\n",
    "                 [0.812239, 0.266786, 0.274661],\n",
    "                 [0.817341, 0.270954, 0.270390],\n",
    "                 [0.822386, 0.275197, 0.266085],\n",
    "                 [0.827372, 0.279517, 0.261750],\n",
    "                 [0.832299, 0.283913, 0.257383],\n",
    "                 [0.837165, 0.288385, 0.252988],\n",
    "                 [0.841969, 0.292933, 0.248564],\n",
    "                 [0.846709, 0.297559, 0.244113],\n",
    "                 [0.851384, 0.302260, 0.239636],\n",
    "                 [0.855992, 0.307038, 0.235133],\n",
    "                 [0.860533, 0.311892, 0.230606],\n",
    "                 [0.865006, 0.316822, 0.226055],\n",
    "                 [0.869409, 0.321827, 0.221482],\n",
    "                 [0.873741, 0.326906, 0.216886],\n",
    "                 [0.878001, 0.332060, 0.212268],\n",
    "                 [0.882188, 0.337287, 0.207628],\n",
    "                 [0.886302, 0.342586, 0.202968],\n",
    "                 [0.890341, 0.347957, 0.198286],\n",
    "                 [0.894305, 0.353399, 0.193584],\n",
    "                 [0.898192, 0.358911, 0.188860],\n",
    "                 [0.902003, 0.364492, 0.184116],\n",
    "                 [0.905735, 0.370140, 0.179350],\n",
    "                 [0.909390, 0.375856, 0.174563],\n",
    "                 [0.912966, 0.381636, 0.169755],\n",
    "                 [0.916462, 0.387481, 0.164924],\n",
    "                 [0.919879, 0.393389, 0.160070],\n",
    "                 [0.923215, 0.399359, 0.155193],\n",
    "                 [0.926470, 0.405389, 0.150292],\n",
    "                 [0.929644, 0.411479, 0.145367],\n",
    "                 [0.932737, 0.417627, 0.140417],\n",
    "                 [0.935747, 0.423831, 0.135440],\n",
    "                 [0.938675, 0.430091, 0.130438],\n",
    "                 [0.941521, 0.436405, 0.125409],\n",
    "                 [0.944285, 0.442772, 0.120354],\n",
    "                 [0.946965, 0.449191, 0.115272],\n",
    "                 [0.949562, 0.455660, 0.110164],\n",
    "                 [0.952075, 0.462178, 0.105031],\n",
    "                 [0.954506, 0.468744, 0.099874],\n",
    "                 [0.956852, 0.475356, 0.094695],\n",
    "                 [0.959114, 0.482014, 0.089499],\n",
    "                 [0.961293, 0.488716, 0.084289],\n",
    "                 [0.963387, 0.495462, 0.079073],\n",
    "                 [0.965397, 0.502249, 0.073859],\n",
    "                 [0.967322, 0.509078, 0.068659],\n",
    "                 [0.969163, 0.515946, 0.063488],\n",
    "                 [0.970919, 0.522853, 0.058367],\n",
    "                 [0.972590, 0.529798, 0.053324],\n",
    "                 [0.974176, 0.536780, 0.048392],\n",
    "                 [0.975677, 0.543798, 0.043618],\n",
    "                 [0.977092, 0.550850, 0.039050],\n",
    "                 [0.978422, 0.557937, 0.034931],\n",
    "                 [0.979666, 0.565057, 0.031409],\n",
    "                 [0.980824, 0.572209, 0.028508],\n",
    "                 [0.981895, 0.579392, 0.026250],\n",
    "                 [0.982881, 0.586606, 0.024661],\n",
    "                 [0.983779, 0.593849, 0.023770],\n",
    "                 [0.984591, 0.601122, 0.023606],\n",
    "                 [0.985315, 0.608422, 0.024202],\n",
    "                 [0.985952, 0.615750, 0.025592],\n",
    "                 [0.986502, 0.623105, 0.027814],\n",
    "                 [0.986964, 0.630485, 0.030908],\n",
    "                 [0.987337, 0.637890, 0.034916],\n",
    "                 [0.987622, 0.645320, 0.039886],\n",
    "                 [0.987819, 0.652773, 0.045581],\n",
    "                 [0.987926, 0.660250, 0.051750],\n",
    "                 [0.987945, 0.667748, 0.058329],\n",
    "                 [0.987874, 0.675267, 0.065257],\n",
    "                 [0.987714, 0.682807, 0.072489],\n",
    "                 [0.987464, 0.690366, 0.079990],\n",
    "                 [0.987124, 0.697944, 0.087731],\n",
    "                 [0.986694, 0.705540, 0.095694],\n",
    "                 [0.986175, 0.713153, 0.103863],\n",
    "                 [0.985566, 0.720782, 0.112229],\n",
    "                 [0.984865, 0.728427, 0.120785],\n",
    "                 [0.984075, 0.736087, 0.129527],\n",
    "                 [0.983196, 0.743758, 0.138453],\n",
    "                 [0.982228, 0.751442, 0.147565],\n",
    "                 [0.981173, 0.759135, 0.156863],\n",
    "                 [0.980032, 0.766837, 0.166353],\n",
    "                 [0.978806, 0.774545, 0.176037],\n",
    "                 [0.977497, 0.782258, 0.185923],\n",
    "                 [0.976108, 0.789974, 0.196018],\n",
    "                 [0.974638, 0.797692, 0.206332],\n",
    "                 [0.973088, 0.805409, 0.216877],\n",
    "                 [0.971468, 0.813122, 0.227658],\n",
    "                 [0.969783, 0.820825, 0.238686],\n",
    "                 [0.968041, 0.828515, 0.249972],\n",
    "                 [0.966243, 0.836191, 0.261534],\n",
    "                 [0.964394, 0.843848, 0.273391],\n",
    "                 [0.962517, 0.851476, 0.285546],\n",
    "                 [0.960626, 0.859069, 0.298010],\n",
    "                 [0.958720, 0.866624, 0.310820],\n",
    "                 [0.956834, 0.874129, 0.323974],\n",
    "                 [0.954997, 0.881569, 0.337475],\n",
    "                 [0.953215, 0.888942, 0.351369],\n",
    "                 [0.951546, 0.896226, 0.365627],\n",
    "                 [0.950018, 0.903409, 0.380271],\n",
    "                 [0.948683, 0.910473, 0.395289],\n",
    "                 [0.947594, 0.917399, 0.410665],\n",
    "                 [0.946809, 0.924168, 0.426373],\n",
    "                 [0.946392, 0.930761, 0.442367],\n",
    "                 [0.946403, 0.937159, 0.458592],\n",
    "                 [0.946903, 0.943348, 0.474970],\n",
    "                 [0.947937, 0.949318, 0.491426],\n",
    "                 [0.949545, 0.955063, 0.507860],\n",
    "                 [0.951740, 0.960587, 0.524203],\n",
    "                 [0.954529, 0.965896, 0.540361],\n",
    "                 [0.957896, 0.971003, 0.556275],\n",
    "                 [0.961812, 0.975924, 0.571925],\n",
    "                 [0.966249, 0.980678, 0.587206],\n",
    "                 [0.971162, 0.985282, 0.602154],\n",
    "                 [0.976511, 0.989753, 0.616760],\n",
    "                 [0.982257, 0.994109, 0.631017],\n",
    "                 [0.988362, 0.998364, 0.644924]]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "inferno = ListedColormap(_inferno_data, name='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "_viridis_data = [[0.267004, 0.004874, 0.329415],\n",
    "                 [0.268510, 0.009605, 0.335427],\n",
    "                 [0.269944, 0.014625, 0.341379],\n",
    "                 [0.271305, 0.019942, 0.347269],\n",
    "                 [0.272594, 0.025563, 0.353093],\n",
    "                 [0.273809, 0.031497, 0.358853],\n",
    "                 [0.274952, 0.037752, 0.364543],\n",
    "                 [0.276022, 0.044167, 0.370164],\n",
    "                 [0.277018, 0.050344, 0.375715],\n",
    "                 [0.277941, 0.056324, 0.381191],\n",
    "                 [0.278791, 0.062145, 0.386592],\n",
    "                 [0.279566, 0.067836, 0.391917],\n",
    "                 [0.280267, 0.073417, 0.397163],\n",
    "                 [0.280894, 0.078907, 0.402329],\n",
    "                 [0.281446, 0.084320, 0.407414],\n",
    "                 [0.281924, 0.089666, 0.412415],\n",
    "                 [0.282327, 0.094955, 0.417331],\n",
    "                 [0.282656, 0.100196, 0.422160],\n",
    "                 [0.282910, 0.105393, 0.426902],\n",
    "                 [0.283091, 0.110553, 0.431554],\n",
    "                 [0.283197, 0.115680, 0.436115],\n",
    "                 [0.283229, 0.120777, 0.440584],\n",
    "                 [0.283187, 0.125848, 0.444960],\n",
    "                 [0.283072, 0.130895, 0.449241],\n",
    "                 [0.282884, 0.135920, 0.453427],\n",
    "                 [0.282623, 0.140926, 0.457517],\n",
    "                 [0.282290, 0.145912, 0.461510],\n",
    "                 [0.281887, 0.150881, 0.465405],\n",
    "                 [0.281412, 0.155834, 0.469201],\n",
    "                 [0.280868, 0.160771, 0.472899],\n",
    "                 [0.280255, 0.165693, 0.476498],\n",
    "                 [0.279574, 0.170599, 0.479997],\n",
    "                 [0.278826, 0.175490, 0.483397],\n",
    "                 [0.278012, 0.180367, 0.486697],\n",
    "                 [0.277134, 0.185228, 0.489898],\n",
    "                 [0.276194, 0.190074, 0.493001],\n",
    "                 [0.275191, 0.194905, 0.496005],\n",
    "                 [0.274128, 0.199721, 0.498911],\n",
    "                 [0.273006, 0.204520, 0.501721],\n",
    "                 [0.271828, 0.209303, 0.504434],\n",
    "                 [0.270595, 0.214069, 0.507052],\n",
    "                 [0.269308, 0.218818, 0.509577],\n",
    "                 [0.267968, 0.223549, 0.512008],\n",
    "                 [0.266580, 0.228262, 0.514349],\n",
    "                 [0.265145, 0.232956, 0.516599],\n",
    "                 [0.263663, 0.237631, 0.518762],\n",
    "                 [0.262138, 0.242286, 0.520837],\n",
    "                 [0.260571, 0.246922, 0.522828],\n",
    "                 [0.258965, 0.251537, 0.524736],\n",
    "                 [0.257322, 0.256130, 0.526563],\n",
    "                 [0.255645, 0.260703, 0.528312],\n",
    "                 [0.253935, 0.265254, 0.529983],\n",
    "                 [0.252194, 0.269783, 0.531579],\n",
    "                 [0.250425, 0.274290, 0.533103],\n",
    "                 [0.248629, 0.278775, 0.534556],\n",
    "                 [0.246811, 0.283237, 0.535941],\n",
    "                 [0.244972, 0.287675, 0.537260],\n",
    "                 [0.243113, 0.292092, 0.538516],\n",
    "                 [0.241237, 0.296485, 0.539709],\n",
    "                 [0.239346, 0.300855, 0.540844],\n",
    "                 [0.237441, 0.305202, 0.541921],\n",
    "                 [0.235526, 0.309527, 0.542944],\n",
    "                 [0.233603, 0.313828, 0.543914],\n",
    "                 [0.231674, 0.318106, 0.544834],\n",
    "                 [0.229739, 0.322361, 0.545706],\n",
    "                 [0.227802, 0.326594, 0.546532],\n",
    "                 [0.225863, 0.330805, 0.547314],\n",
    "                 [0.223925, 0.334994, 0.548053],\n",
    "                 [0.221989, 0.339161, 0.548752],\n",
    "                 [0.220057, 0.343307, 0.549413],\n",
    "                 [0.218130, 0.347432, 0.550038],\n",
    "                 [0.216210, 0.351535, 0.550627],\n",
    "                 [0.214298, 0.355619, 0.551184],\n",
    "                 [0.212395, 0.359683, 0.551710],\n",
    "                 [0.210503, 0.363727, 0.552206],\n",
    "                 [0.208623, 0.367752, 0.552675],\n",
    "                 [0.206756, 0.371758, 0.553117],\n",
    "                 [0.204903, 0.375746, 0.553533],\n",
    "                 [0.203063, 0.379716, 0.553925],\n",
    "                 [0.201239, 0.383670, 0.554294],\n",
    "                 [0.199430, 0.387607, 0.554642],\n",
    "                 [0.197636, 0.391528, 0.554969],\n",
    "                 [0.195860, 0.395433, 0.555276],\n",
    "                 [0.194100, 0.399323, 0.555565],\n",
    "                 [0.192357, 0.403199, 0.555836],\n",
    "                 [0.190631, 0.407061, 0.556089],\n",
    "                 [0.188923, 0.410910, 0.556326],\n",
    "                 [0.187231, 0.414746, 0.556547],\n",
    "                 [0.185556, 0.418570, 0.556753],\n",
    "                 [0.183898, 0.422383, 0.556944],\n",
    "                 [0.182256, 0.426184, 0.557120],\n",
    "                 [0.180629, 0.429975, 0.557282],\n",
    "                 [0.179019, 0.433756, 0.557430],\n",
    "                 [0.177423, 0.437527, 0.557565],\n",
    "                 [0.175841, 0.441290, 0.557685],\n",
    "                 [0.174274, 0.445044, 0.557792],\n",
    "                 [0.172719, 0.448791, 0.557885],\n",
    "                 [0.171176, 0.452530, 0.557965],\n",
    "                 [0.169646, 0.456262, 0.558030],\n",
    "                 [0.168126, 0.459988, 0.558082],\n",
    "                 [0.166617, 0.463708, 0.558119],\n",
    "                 [0.165117, 0.467423, 0.558141],\n",
    "                 [0.163625, 0.471133, 0.558148],\n",
    "                 [0.162142, 0.474838, 0.558140],\n",
    "                 [0.160665, 0.478540, 0.558115],\n",
    "                 [0.159194, 0.482237, 0.558073],\n",
    "                 [0.157729, 0.485932, 0.558013],\n",
    "                 [0.156270, 0.489624, 0.557936],\n",
    "                 [0.154815, 0.493313, 0.557840],\n",
    "                 [0.153364, 0.497000, 0.557724],\n",
    "                 [0.151918, 0.500685, 0.557587],\n",
    "                 [0.150476, 0.504369, 0.557430],\n",
    "                 [0.149039, 0.508051, 0.557250],\n",
    "                 [0.147607, 0.511733, 0.557049],\n",
    "                 [0.146180, 0.515413, 0.556823],\n",
    "                 [0.144759, 0.519093, 0.556572],\n",
    "                 [0.143343, 0.522773, 0.556295],\n",
    "                 [0.141935, 0.526453, 0.555991],\n",
    "                 [0.140536, 0.530132, 0.555659],\n",
    "                 [0.139147, 0.533812, 0.555298],\n",
    "                 [0.137770, 0.537492, 0.554906],\n",
    "                 [0.136408, 0.541173, 0.554483],\n",
    "                 [0.135066, 0.544853, 0.554029],\n",
    "                 [0.133743, 0.548535, 0.553541],\n",
    "                 [0.132444, 0.552216, 0.553018],\n",
    "                 [0.131172, 0.555899, 0.552459],\n",
    "                 [0.129933, 0.559582, 0.551864],\n",
    "                 [0.128729, 0.563265, 0.551229],\n",
    "                 [0.127568, 0.566949, 0.550556],\n",
    "                 [0.126453, 0.570633, 0.549841],\n",
    "                 [0.125394, 0.574318, 0.549086],\n",
    "                 [0.124395, 0.578002, 0.548287],\n",
    "                 [0.123463, 0.581687, 0.547445],\n",
    "                 [0.122606, 0.585371, 0.546557],\n",
    "                 [0.121831, 0.589055, 0.545623],\n",
    "                 [0.121148, 0.592739, 0.544641],\n",
    "                 [0.120565, 0.596422, 0.543611],\n",
    "                 [0.120092, 0.600104, 0.542530],\n",
    "                 [0.119738, 0.603785, 0.541400],\n",
    "                 [0.119512, 0.607464, 0.540218],\n",
    "                 [0.119423, 0.611141, 0.538982],\n",
    "                 [0.119483, 0.614817, 0.537692],\n",
    "                 [0.119699, 0.618490, 0.536347],\n",
    "                 [0.120081, 0.622161, 0.534946],\n",
    "                 [0.120638, 0.625828, 0.533488],\n",
    "                 [0.121380, 0.629492, 0.531973],\n",
    "                 [0.122312, 0.633153, 0.530398],\n",
    "                 [0.123444, 0.636809, 0.528763],\n",
    "                 [0.124780, 0.640461, 0.527068],\n",
    "                 [0.126326, 0.644107, 0.525311],\n",
    "                 [0.128087, 0.647749, 0.523491],\n",
    "                 [0.130067, 0.651384, 0.521608],\n",
    "                 [0.132268, 0.655014, 0.519661],\n",
    "                 [0.134692, 0.658636, 0.517649],\n",
    "                 [0.137339, 0.662252, 0.515571],\n",
    "                 [0.140210, 0.665859, 0.513427],\n",
    "                 [0.143303, 0.669459, 0.511215],\n",
    "                 [0.146616, 0.673050, 0.508936],\n",
    "                 [0.150148, 0.676631, 0.506589],\n",
    "                 [0.153894, 0.680203, 0.504172],\n",
    "                 [0.157851, 0.683765, 0.501686],\n",
    "                 [0.162016, 0.687316, 0.499129],\n",
    "                 [0.166383, 0.690856, 0.496502],\n",
    "                 [0.170948, 0.694384, 0.493803],\n",
    "                 [0.175707, 0.697900, 0.491033],\n",
    "                 [0.180653, 0.701402, 0.488189],\n",
    "                 [0.185783, 0.704891, 0.485273],\n",
    "                 [0.191090, 0.708366, 0.482284],\n",
    "                 [0.196571, 0.711827, 0.479221],\n",
    "                 [0.202219, 0.715272, 0.476084],\n",
    "                 [0.208030, 0.718701, 0.472873],\n",
    "                 [0.214000, 0.722114, 0.469588],\n",
    "                 [0.220124, 0.725509, 0.466226],\n",
    "                 [0.226397, 0.728888, 0.462789],\n",
    "                 [0.232815, 0.732247, 0.459277],\n",
    "                 [0.239374, 0.735588, 0.455688],\n",
    "                 [0.246070, 0.738910, 0.452024],\n",
    "                 [0.252899, 0.742211, 0.448284],\n",
    "                 [0.259857, 0.745492, 0.444467],\n",
    "                 [0.266941, 0.748751, 0.440573],\n",
    "                 [0.274149, 0.751988, 0.436601],\n",
    "                 [0.281477, 0.755203, 0.432552],\n",
    "                 [0.288921, 0.758394, 0.428426],\n",
    "                 [0.296479, 0.761561, 0.424223],\n",
    "                 [0.304148, 0.764704, 0.419943],\n",
    "                 [0.311925, 0.767822, 0.415586],\n",
    "                 [0.319809, 0.770914, 0.411152],\n",
    "                 [0.327796, 0.773980, 0.406640],\n",
    "                 [0.335885, 0.777018, 0.402049],\n",
    "                 [0.344074, 0.780029, 0.397381],\n",
    "                 [0.352360, 0.783011, 0.392636],\n",
    "                 [0.360741, 0.785964, 0.387814],\n",
    "                 [0.369214, 0.788888, 0.382914],\n",
    "                 [0.377779, 0.791781, 0.377939],\n",
    "                 [0.386433, 0.794644, 0.372886],\n",
    "                 [0.395174, 0.797475, 0.367757],\n",
    "                 [0.404001, 0.800275, 0.362552],\n",
    "                 [0.412913, 0.803041, 0.357269],\n",
    "                 [0.421908, 0.805774, 0.351910],\n",
    "                 [0.430983, 0.808473, 0.346476],\n",
    "                 [0.440137, 0.811138, 0.340967],\n",
    "                 [0.449368, 0.813768, 0.335384],\n",
    "                 [0.458674, 0.816363, 0.329727],\n",
    "                 [0.468053, 0.818921, 0.323998],\n",
    "                 [0.477504, 0.821444, 0.318195],\n",
    "                 [0.487026, 0.823929, 0.312321],\n",
    "                 [0.496615, 0.826376, 0.306377],\n",
    "                 [0.506271, 0.828786, 0.300362],\n",
    "                 [0.515992, 0.831158, 0.294279],\n",
    "                 [0.525776, 0.833491, 0.288127],\n",
    "                 [0.535621, 0.835785, 0.281908],\n",
    "                 [0.545524, 0.838039, 0.275626],\n",
    "                 [0.555484, 0.840254, 0.269281],\n",
    "                 [0.565498, 0.842430, 0.262877],\n",
    "                 [0.575563, 0.844566, 0.256415],\n",
    "                 [0.585678, 0.846661, 0.249897],\n",
    "                 [0.595839, 0.848717, 0.243329],\n",
    "                 [0.606045, 0.850733, 0.236712],\n",
    "                 [0.616293, 0.852709, 0.230052],\n",
    "                 [0.626579, 0.854645, 0.223353],\n",
    "                 [0.636902, 0.856542, 0.216620],\n",
    "                 [0.647257, 0.858400, 0.209861],\n",
    "                 [0.657642, 0.860219, 0.203082],\n",
    "                 [0.668054, 0.861999, 0.196293],\n",
    "                 [0.678489, 0.863742, 0.189503],\n",
    "                 [0.688944, 0.865448, 0.182725],\n",
    "                 [0.699415, 0.867117, 0.175971],\n",
    "                 [0.709898, 0.868751, 0.169257],\n",
    "                 [0.720391, 0.870350, 0.162603],\n",
    "                 [0.730889, 0.871916, 0.156029],\n",
    "                 [0.741388, 0.873449, 0.149561],\n",
    "                 [0.751884, 0.874951, 0.143228],\n",
    "                 [0.762373, 0.876424, 0.137064],\n",
    "                 [0.772852, 0.877868, 0.131109],\n",
    "                 [0.783315, 0.879285, 0.125405],\n",
    "                 [0.793760, 0.880678, 0.120005],\n",
    "                 [0.804182, 0.882046, 0.114965],\n",
    "                 [0.814576, 0.883393, 0.110347],\n",
    "                 [0.824940, 0.884720, 0.106217],\n",
    "                 [0.835270, 0.886029, 0.102646],\n",
    "                 [0.845561, 0.887322, 0.099702],\n",
    "                 [0.855810, 0.888601, 0.097452],\n",
    "                 [0.866013, 0.889868, 0.095953],\n",
    "                 [0.876168, 0.891125, 0.095250],\n",
    "                 [0.886271, 0.892374, 0.095374],\n",
    "                 [0.896320, 0.893616, 0.096335],\n",
    "                 [0.906311, 0.894855, 0.098125],\n",
    "                 [0.916242, 0.896091, 0.100717],\n",
    "                 [0.926106, 0.897330, 0.104071],\n",
    "                 [0.935904, 0.898570, 0.108131],\n",
    "                 [0.945636, 0.899815, 0.112838],\n",
    "                 [0.955300, 0.901065, 0.118128],\n",
    "                 [0.964894, 0.902323, 0.123941],\n",
    "                 [0.974417, 0.903590, 0.130215],\n",
    "                 [0.983868, 0.904867, 0.136897],\n",
    "                 [0.993248, 0.906157, 0.143936]]\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "viridis = ListedColormap(_viridis_data, name='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
