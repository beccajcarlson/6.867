{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Grad Descent\n",
    "# import modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.1 define grad descent function\n",
    "def grad_descent(x_init, alpha, thresh, f, f_prime):\n",
    "    iter = 1\n",
    "    prev = x_init\n",
    "    next = x_init - alpha*f_prime(prev)\n",
    "    \n",
    "    while(sum(abs(prev-next)) >= thresh):\n",
    "        prev = next\n",
    "        next = prev - alpha*f_prime(prev)\n",
    "        iter += 1\n",
    "        \n",
    "    return np.append(next, f(next))\n",
    "    #print \"Iterations = \", iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import data for grad descent\n",
    "import pylab as pl\n",
    "\n",
    "def getData():\n",
    "\n",
    "    # load the parameters for the negative Gaussian function and quadratic bowl function\n",
    "    # return a tuple that contains parameters for Gaussian mean, Gaussian covariance,\n",
    "    # A and b for quadratic bowl in order\n",
    "\n",
    "    data = pl.loadtxt('/Users/becca/Documents/Google Drive/Dropbox/Grad School/ML/code_hw1/P1/parametersp1.txt')\n",
    "\n",
    "    gaussMean = data[0,:]\n",
    "    gaussCov = data[1:3,:]\n",
    "\n",
    "    quadBowlA = data[3:5,:]\n",
    "    quadBowlb = data[5,:]\n",
    "\n",
    "    return (gaussMean,gaussCov,quadBowlA,quadBowlb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define variables for gaussian and quadratic bowl functions\n",
    "gaussMean = getData()[0]\n",
    "gaussCov = getData()[1]\n",
    "n = gaussMean.shape[0]\n",
    "quadBowlA = getData()[2]\n",
    "quadBowlB = getData()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 33.335,  16.67 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quadratic Bowl functions\n",
    "f = lambda x: 0.5 * np.dot(np.dot(x, quadBowlA), x.T) - np.dot(x, quadBowlB)\n",
    "f_prime = lambda x: np.dot(quadBowlA, x.T) - quadBowlB\n",
    "x = np.array([30, 26.667])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gaussian functions\n",
    "from operator import truediv\n",
    "f = lambda x: -10**4/((2*math.pi)**n*np.linalg.det(gaussCov))**(1/2)*np.exp(-1/2*np.dot(np.dot((x-gaussMean).T,inv(gaussCov)),(x-gaussMean)))\n",
    "f_prime = lambda x: np.dot(np.dot(-f(x), inv(gaussCov)), (x-gaussMean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.2. Numerical Gradient Evaluation\n",
    "# Gaussian functions\n",
    "f = lambda x: -10**4/((2*math.pi)**n*np.linalg.det(gaussCov))**(1/2)*np.exp(-1/2*np.dot(np.dot((x-gaussMean).T,inv(gaussCov)),(x-gaussMean)))\n",
    "delta = np.array([1e-10, 1e-10])\n",
    "f_prime = lambda x: np.array([((f(x[0]+delta)-f(x[0]))/delta)[0], ((f(x[1]+delta)-f(x[1]))/delta)[1]])\n",
    "x = np.array([10, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to do numerical gradient evaluation for quadratic bowl!!\n",
    "f = lambda x: -10**4/((2*math.pi)**n*np.linalg.det(gaussCov))**(1/2)*np.exp(-1/2*np.dot(np.dot((x-gaussMean).T,inv(gaussCov)),(x-gaussMean)))\n",
    "delta = np.array([1e-10, 1e-10])\n",
    "# just use f_prime numerical grad for Gaussian, should be the same or very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.3. Stochastic Gradient Descent\n",
    "# retrieve data\n",
    "def getData():\n",
    "    \n",
    "    # load the fitting data for X and y and return as elements of a tuple\n",
    "    # X is a 100 by 10 matrix and y is a vector of length 100\n",
    "    # Each corresponding row for X and y represents a single data sample\n",
    "\n",
    "    xp1 = pl.loadtxt('/Users/becca/GitHub/6.867/code_hw1/P1/fittingdatap1_x.txt')\n",
    "    yp1 = pl.loadtxt('/Users/becca/GitHub/6.867/code_hw1/P1/fittingdatap1_y.txt')\n",
    "\n",
    "    return (xp1,yp1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.5032541 ,  -2.3367948 ,  -6.31670032,   6.81230107,\n",
       "        -1.06337989,   6.67469398,   3.4118044 ,  -0.45573592,\n",
       "       -12.94593466,  15.73289812])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define variables\n",
    "xp1 = getData()[0]\n",
    "yp1 = getData()[1]\n",
    "\n",
    "# closed form soln:\n",
    "w = np.dot(np.dot(inv(np.dot(xp1.T, xp1)), xp1.T), yp1)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.36668948e-11,  -3.18711949e-11,  -9.22887588e-11,\n",
       "        -1.64776378e-10,   1.03009487e-10,  -8.93619715e-11,\n",
       "        -6.25343613e-11,  -5.73104542e-11,  -4.17318354e-10,\n",
       "        -5.15504244e-10])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define LMS function and derivative\n",
    "f = lambda w: sum((np.dot(xp1,w)-yp1)**2)\n",
    "f_prime = lambda w: (2*np.dot((np.dot(xp1,w)-yp1), xp1))\n",
    "f_prime(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.5032541 ,  -1.3367948 ,  -5.31670032,   7.81230107,\n",
       "        -0.06337989,   7.67469398,   4.4118044 ,   0.54426408,\n",
       "       -11.94593466,  16.73289812])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define initial value for test\n",
    "w_init = w+1\n",
    "w_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define grad descent with modified obj criterion\n",
    "def grad_descent(x_init, alpha, thresh, f, f_prime):\n",
    "    iter = 1\n",
    "    prev = x_init\n",
    "    next = x_init - alpha*f_prime(prev)\n",
    "    \n",
    "    while(sum(abs(f_prime(next)))) >= thresh:\n",
    "        prev = next\n",
    "        next = prev - alpha*f_prime(prev)\n",
    "        iter += 1\n",
    "        \n",
    "    return np.append(next, f(next))\n",
    "    #print \"Iterations = \", iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.03239942e-01,  -2.33675857e+00,  -6.31672875e+00,\n",
       "         6.81230347e+00,  -1.06335617e+00,   6.67473759e+00,\n",
       "         3.41189702e+00,  -4.55730185e-01,  -1.29459496e+01,\n",
       "         1.57328925e+01,   8.33321452e+03])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "# working now but with poor convergence\n",
    "w_init = w+1\n",
    "\n",
    "grad_descent(w_init, 1e-8, 12, f, f_prime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent - define function with input i = index of data for gradient\n",
    "# This is working\n",
    "f = lambda w: sum((np.dot(xp1,w)-yp1)**2)\n",
    "f_prime = lambda w, i: (2*np.dot((np.dot(xp1[i],w)-yp1[i]), xp1[i]))\n",
    "f_prime(w, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent function - not working\n",
    "# nrounds - define rounds of data to iterate through\n",
    "\n",
    "def stoch_grad_descent(x_init, alpha, thresh, f, f_prime, nrounds):\n",
    "    i=0\n",
    "    iter = 1\n",
    "    prev = x_init\n",
    "    next = x_init - alpha*f_prime(prev, i)\n",
    "    \n",
    "    for j in range(0, (nrounds-1)):\n",
    "        for i in range(0, 99):\n",
    "            prev = next\n",
    "            next = prev - alpha*f_prime(prev, i)\n",
    "            i+=1\n",
    "            iter += 1\n",
    "            if(sum(abs(f_prime(next, range(0, 99))))) <= thresh:\n",
    "                return np.append(next, f(next)), iter\n",
    "                #print \"Iterations = \", iter\n",
    "        j+=1\n",
    "    if(sum(abs(f_prime(next, range(0, 99))))) <= thresh:\n",
    "        return np.append(next, f(next)), iter\n",
    "    else:\n",
    "        return \"Did not converge, min cost was: \", sum(abs(f_prime(next, range(0, 99)))), iter\n",
    "                #return \"Not converged\"\n",
    "        \n",
    "    #return np.append(next, f(next)), iter\n",
    "    #print \"Iterations = \", iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing part of stoch grad descent loop to troubleshoot\n",
    "# not working...\n",
    "x_init = w\n",
    "i=0\n",
    "alpha = 1e-4\n",
    "iter = 1\n",
    "prev = x_init\n",
    "next = x_init - alpha*f_prime(prev, i)\n",
    "next\n",
    "thresh = 1e-2\n",
    "for j in range(0, 2):\n",
    "       for i in range(0, 99):\n",
    "        prev = next\n",
    "            i +=1\n",
    "            iter += 1\n",
    "                \n",
    "# np.append(next, f(next))\n",
    "# next\n",
    "# sum(abs(f_prime(next, range(0,100))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test SGD function\n",
    "stoch_grad_descent(w+0.0001, 1e-20, 2000, f, f_prime, 1000)\n",
    "# not converging.. is iterating but seems like cost not decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem 2.\n",
    "# Linear Basis Function Regression\n",
    "# import data and plot\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "def getData(ifPlotData=True):\n",
    "    # load the fitting data and (optionally) plot out for examination\n",
    "    # return the X and Y as a tuple\n",
    "\n",
    "    data = pl.loadtxt('/Users/becca/GitHub/6.867/code_hw1/P2/curvefittingp2.txt')\n",
    "\n",
    "    X = data[0,:]\n",
    "    Y = data[1,:]\n",
    "\n",
    "    if ifPlotData:\n",
    "        plt.plot(X,Y,'o')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.show()\n",
    "\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define variables\n",
    "X = getData(ifPlotData = False)[0]\n",
    "Y = getData(ifPlotData = False)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xplot = X[np.newaxis]\n",
    "Xplot.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define polyx function - takes x and degree, returns array of x's up to given degree, including added col of ones\n",
    "def polyx(Xplot, degree):\n",
    "    len = Xplot.shape[1]\n",
    "    if degree in range(0,1):\n",
    "        Xnew = np.ones((len,1))\n",
    "        return Xnew\n",
    "    elif degree in range(1,2):\n",
    "        Xnew = np.hstack((np.ones((len, 1)), Xplot.T))\n",
    "        return Xnew\n",
    "    else:\n",
    "        Xnew = np.hstack((np.ones((len, 1)), Xplot.T))\n",
    "        for i in range(2, degree+1):\n",
    "            Xnew = np.hstack((Xnew, Xplot.T**i))\n",
    "            i += 1\n",
    "            \n",
    "        return Xnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.1. Polynomial Fit\n",
    "# Note: code for fits is the same, change m only\n",
    "# m = 0\n",
    "m = 0\n",
    "Xplot = np.array(np.linspace(0,1,num = 110)).T[np.newaxis]\n",
    "Yplot = np.dot(polyx(Xplot, m),np.polyfit(X, Y, m)[np.newaxis].T[::-1])\n",
    "Yactual = np.cos(math.pi*Xplot) + np.cos(2*math.pi*Xplot)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "plt.plot(Xplot.T, Yplot, '-')\n",
    "plt.plot(Xplot.T, Yactual.T, '-')\n",
    "plt.plot(X, Y, 'o')\n",
    "plt.plot()\n",
    "plt.title(\"Linear Regression (M = %s)\" %(m))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([-2,3])\n",
    "fig.savefig('2.1 poly %s.jpg' %(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m = 1\n",
    "m = 1\n",
    "Xplot = np.array(np.linspace(0,1,num = 110)).T[np.newaxis]\n",
    "Yplot = np.dot(polyx(Xplot, m),np.polyfit(X, Y, m)[np.newaxis].T[::-1])\n",
    "Yactual = np.cos(math.pi*Xplot) + np.cos(2*math.pi*Xplot)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "plt.plot(Xplot.T, Yplot, '-')\n",
    "plt.plot(Xplot.T, Yactual.T, '-')\n",
    "plt.plot(X, Y, 'o')\n",
    "plt.plot()\n",
    "plt.title(\"Linear Regression (M = %s)\" %(m))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([-2,3])\n",
    "fig.savefig('2.1 poly %s.jpg' %(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m = 2\n",
    "m = 2\n",
    "Xplot = np.array(np.linspace(0,1,num = 110)).T[np.newaxis]\n",
    "Yplot = np.dot(polyx(Xplot, m),np.polyfit(X, Y, m)[np.newaxis].T[::-1])\n",
    "Yactual = np.cos(math.pi*Xplot) + np.cos(2*math.pi*Xplot)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "plt.plot(Xplot.T, Yplot, '-')\n",
    "plt.plot(Xplot.T, Yactual.T, '-')\n",
    "plt.plot(X, Y, 'o')\n",
    "plt.plot()\n",
    "plt.title(\"Linear Regression (M = %s)\" %(m))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([-2,3])\n",
    "fig.savefig('2.1 poly %s.jpg' %(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m = 10\n",
    "m = 10\n",
    "Xplot = np.array(np.linspace(0,1,num = 110)).T[np.newaxis]\n",
    "Yplot = np.dot(polyx(Xplot, m),np.polyfit(X, Y, m)[np.newaxis].T[::-1])\n",
    "Yactual = np.cos(math.pi*Xplot) + np.cos(2*math.pi*Xplot)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "plt.plot(Xplot.T, Yplot, '-')\n",
    "plt.plot(Xplot.T, Yactual.T, '-')\n",
    "plt.plot(X, Y, 'o')\n",
    "plt.plot()\n",
    "plt.title(\"Linear Regression (M = %s)\" %(m))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([-3,3])\n",
    "fig.savefig('2.1 poly %s.jpg' %(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.2 Compute SSE given degree\n",
    "Xplot = X[np.newaxis]\n",
    "Y = np.reshape(Y, (11,1))\n",
    "f = lambda w, m: 0.5*sum((np.dot(polyx(Xplot, m), w)-Y)**2)\n",
    "f_prime = lambda w, m: np.dot(polyx(Xplot, m).T,(np.dot(polyx(Xplot, m), w)-Y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "range() integer step argument expected, got float.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-741-3db94f646747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: range() integer step argument expected, got float."
     ]
    }
   ],
   "source": [
    "for i in range(0, 1, 1):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sse = []\n",
    "for m in [0,1,2,10]:\n",
    "    w=np.polyfit(X, Y, m)[::-1]\n",
    "    sse.append(f(w, m))\n",
    "    \n",
    "sse\n",
    "\n",
    "ssecontin = []\n",
    "for i in range(0, 11, 1):\n",
    "    w=np.polyfit(X, Y, i)[::-1]\n",
    "    ssecontin.append(f(w, i))\n",
    "    \n",
    "# Plot sse vs m\n",
    "m = [0,1,2,10]\n",
    "mcontin = range(0, 11, 1)\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "\n",
    "plt.plot(mcontin, ssecontin, '-')\n",
    "plt.plot(m, sse, 'o')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('SSE')\n",
    "fig.savefig('2.2 Poly deg vs SSE.jpg' %(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_prime = lambda w, m: np.dot(polyx(Xplot, m).T,(np.dot(polyx(Xplot, m), w)-Y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.9309915])"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate W max likelihood analytically, confirm SSE(w, m) is working\n",
    "m=1\n",
    "Xplot = X[np.newaxis]\n",
    "w=np.polyfit(X, Y, m)[::-1]\n",
    "\n",
    "Y = np.reshape(Y, (11,1))\n",
    ".5*sum((np.dot(polyx(Xplot, m), w)-Y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numerical Deriv Confirmation\n",
    "f = lambda w, m: 0.5*sum((np.dot(polyx(Xplot, m), w)-Y)**2)\n",
    "def num_sse(coeff, m, delta):\n",
    "    grad = []\n",
    "    for i in range(0, len(coeff)):\n",
    "        tmp = np.array(list(coeff))\n",
    "        deltmp = np.array(list(coeff))\n",
    "        deltmp[i] = deltmp[i]+delta\n",
    "        grad.append((f(deltmp, m)-f(tmp, m))/delta)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 22.000055])]"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sse(w+2, 0, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 22.]])"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_prime(w+2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot analytical vs numerical, m = 10\n",
    "m = 10\n",
    "w=np.polyfit(X, Y, m)[::-1]\n",
    "actual = f_prime(w+2, m)\n",
    "num10 = num_sse(w+2, m, 0.5)\n",
    "num100 = num_sse(w+2, m, 5)\n",
    "num1 = num_sse(w+2, m, 1)\n",
    "\n",
    "# x = linspace(25, 75, num = 200)\n",
    "# y = x*slope + intercept\n",
    "\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "# plt.plot(Xplot.T, Yplot, '-')\n",
    "# plt.plot(Xplot.T, Yactual.T, '-')\n",
    "plt.plot(actual, num10, '-', label = '0.5')\n",
    "plt.plot(actual, num1, '-', label = '1')\n",
    "plt.plot(actual, num100, '-', label = '5')\n",
    "plt.plot(actual, actual, '--')\n",
    "plt.title(\"M = %s\"%(m))\n",
    "plt.xlabel('Analytical Gradient')\n",
    "plt.ylabel('Numerical Derivative')\n",
    "plt.legend(loc='upper left', title = 'Step size')\n",
    "fig.savefig('2.2 Anal vs Numer %s.jpg' %(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot analytical vs numerical, m = 1\n",
    "m = 1\n",
    "w=np.polyfit(X, Y, m)[::-1]\n",
    "actual = f_prime(w+2, m)\n",
    "num10 = num_sse(w+2, m, 0.5)\n",
    "num100 = num_sse(w+2, m, 5)\n",
    "num1 = num_sse(w+2, m, 1)\n",
    "\n",
    "# x = linspace(25, 75, num = 200)\n",
    "# y = x*slope + intercept\n",
    "\n",
    "fig = plt.figure(figsize=(5,5), dpi=300)\n",
    "plt.plot(actual, num10, '-', label = '0.5')\n",
    "plt.plot(actual, num1, '-', label = '1')\n",
    "plt.plot(actual, num100, '-', label = '5')\n",
    "plt.plot(actual, actual, '--')\n",
    "plt.title(\"M = %s\"%(m))\n",
    "plt.xlabel('Analytical Gradient')\n",
    "plt.ylabel('Numerical Derivative')\n",
    "plt.legend(loc='upper left', title = 'Step size')\n",
    "fig.savefig('2.2 Anal vs Numer %s.jpg' %(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.3 Define batch grad descent function for polynomial:\n",
    "def grad_descent_multi(w_init, alpha, thresh, f, f_prime, m):\n",
    "    iter = 1\n",
    "    prev = w_init\n",
    "    next = w_init - alpha*f_prime(prev, m)\n",
    "    \n",
    "    while(sum(abs(f_prime(next, m)))) >= thresh:\n",
    "        prev = next\n",
    "        next = prev - alpha*f_prime(prev, m)\n",
    "        iter += 1\n",
    "        \n",
    "    return np.append(next, f(next, m))\n",
    "    #print \"Iterations = \", iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5bf0493bddb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test function - works!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgrad_descent_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Test function - works!\n",
    "w=np.polyfit(X, Y, 2)[::-1]\n",
    "grad_descent_multi(w+3, .01, 1e-5, f, f_prime, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.3 Continued. Need to test with SGD!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.45620123, -12.15055147,  10.33771957,   0.34643172])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.4 Cosine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
